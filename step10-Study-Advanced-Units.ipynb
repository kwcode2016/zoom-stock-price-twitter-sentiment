{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Study Advanced Units (NLP)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The reason for using NLP techniques in my current twitter sentiment project is because traditional machine learning techniques are not able to handle the vast amount of variance in natural language. \n",
    "\n",
    "And even with NLP, older NLP techniques (e.g. Bag of words, TF-IDF, LSA) won't be too accurate for sentiment analysis.\n",
    "\n",
    "In this notebook, we will explore 4 sections and have a sections:\n",
    "- classic Machine Learning techniques and where they fall short\n",
    "- Older NLP Techniques\n",
    "- Current State of the Art Transformer Techniques\n",
    "- The Future of NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Traditional Machine Learning\n",
    "\n",
    "Traditional machine learning and Natural Language Processing (NLP) are two distinct fields within the broader scope of artificial intelligence. Traditional machine learning focuses on the development of algorithms that can learn from and make predictions or decisions based on data. These algorithms can be applied to a wide range of applications, such as image recognition, spam filtering, and recommendation systems. Techniques employed in traditional machine learning include linear regression, decision trees, support vector machines, and neural networks, among others. The success of traditional machine learning largely depends on the quality and representation of the data, as well as the choice of the appropriate algorithm for a given task.\n",
    "\n",
    "But there is a problem with Traditional Machine Learning when it comes to NLP (especially with sentiment analysis which this project is about). These limitations arise from the unique challenges posed by NLP:\n",
    "\n",
    "1. Sequential and contextual nature of language: Language is inherently sequential, and the meaning of words often depends on the context in which they appear. Traditional machine learning algorithms do not naturally capture the order or context of words, which can lead to poor performance in understanding the sentiment.\n",
    "2. High-dimensional and sparse data: Text data is typically represented as high-dimensional and sparse feature vectors, where each dimension corresponds to a specific word or n-gram. Traditional machine learning algorithms can struggle to effectively learn from such data, as they are not designed to handle high-dimensional, sparse inputs.\n",
    "3. Lack of semantic understanding: Traditional machine learning algorithms rely on handcrafted features and do not inherently understand the semantics of words or phrases. This makes it difficult for them to capture more complex linguistic structures and relationships, such as negations, sarcasm, or idioms, which are crucial for sentiment analysis.\n",
    "4. Feature engineering: Traditional machine learning algorithms require extensive feature engineering to transform raw text data into a suitable input format. This can be time-consuming and may require expert domain knowledge. Moreover, the quality of the features directly affects the performance of the models, making it challenging to achieve high accuracy without carefully crafted features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Natural Language Processing (NLP)\n",
    "\n",
    "In contrast, NLP is a subfield of artificial intelligence that specifically deals with the interaction between computers and human language. It aims to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. NLP encompasses various tasks, such as sentiment analysis, machine translation, summarization, question answering, and named entity recognition. Traditional machine learning techniques can be applied to NLP tasks; however, they often require extensive feature engineering, which involves manually selecting relevant features from raw text data to better represent the input for the learning algorithms.\n",
    "\n",
    "\n",
    "# Original NLP methods\n",
    "\n",
    "Before the advent of deep learning and neural networks, several non-neural network NLP techniques were widely used to address various natural language processing tasks. Some of the top non-neural network NLP methods include:\n",
    "\n",
    "1.Bag of Words (BoW) Model: The Bag of Words model is a simple text representation technique that creates a document-term matrix by counting the frequency of words in a document. It ignores word order and focuses only on the presence or absence of terms in the text. This model is often used in conjunction with machine learning algorithms such as Naive Bayes, logistic regression, and support vector machines for tasks like text classification, sentiment analysis, and spam filtering. \n",
    "\n",
    "2.Term Frequency-Inverse Document Frequency (TF-IDF): TF-IDF is a numerical statistic that reflects the importance of a word in a document within a collection of documents. It is calculated by multiplying the term frequency (how often a word appears in a document) by the inverse document frequency (a measure of how rare the word is across the entire document collection). TF-IDF helps to weigh down common words while giving more importance to words that are more relevant to the document's content. It is often used with traditional machine learning algorithms for information retrieval, text classification, and clustering tasks. \n",
    "\n",
    "3. Latent Semantic Analysis (LSA): LSA, also known as Latent Semantic Indexing, is a technique for reducing the dimensionality of text data by mapping words and documents to a lower-dimensional space. It employs singular value decomposition (SVD) to capture the underlying semantic structure of the text. LSA can be used for tasks such as document similarity, text summarization, and topic modeling. \n",
    "\n",
    "4.Part-of-Speech (POS) Tagging and Named Entity Recognition (NER): Part-of-speech tagging is the process of assigning grammatical categories (such as nouns, verbs, adjectives, etc.) to words in a text, while Named Entity Recognition is a technique for identifying and classifying entities (such as people, organizations, locations, etc.) within the text. Both POS tagging and NER can be performed using rule-based systems, decision trees, Hidden Markov Models (HMMs), and Conditional Random Fields (CRFs), among other non-neural network methods. These techniques are crucial for tasks such as information extraction, relation extraction, and question answering.\n",
    "\n",
    "These methods have the same issue of being limited to what training data they are exposed to. Also, accuracy depends on WHAT the models are asked to classify. They are too rigid. As soon as what we need to ask the model changes, accuracy drops considerably and we have to retrain the entire model again with the right data. And in the case of sentiment, this is a big issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Current State of the Art Transformer Techniques\n",
    "\n",
    "\n",
    "Deep learning has revolutionized NLP in recent years, leading to the development of powerful models that can learn complex representations of text data without the need for manual feature engineering. One of the most significant breakthroughs in NLP came with the advent of transformer-based models, such as BERT, GPT, and RoBERTa. These models employ the transformer architecture, which relies on self-attention mechanisms to capture long-range dependencies and contextual information within the input text.\n",
    "\n",
    "Transformers have significantly outperformed previous NLP techniques on a wide range of tasks, setting new state-of-the-art benchmarks in the process. The key advantage of transformer-based models is their ability to learn contextualized word embeddings, which enables them to capture subtle nuances and variations in meaning that were previously difficult for traditional machine learning techniques to model. Moreover, transformers are highly scalable, allowing them to be trained on massive amounts of data, leading to better generalization and performance on various NLP tasks.\n",
    "\n",
    "For example, in this project, I am using the Cardiff NLP RoBERTa based model trained on 124M tweets and finetuned for sentiment analysis with TweetEval. A large amount of financial resources and time was used to create this model, and I am able to use this with finetuning to financial tweets for my purposes to create this project to analyze tweets with regards to the stock price of the ZOOM stock. This is quite revolutionary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: The Future of NLP\n",
    "\n",
    "\n",
    "The current state of the art NLP, Large Language Models based on Transformers is very exciting and is a breakthrough, but in essence, these models like BERT, GPT etc, do not possess actual understanding of language or topics in the way humans do. These models rely on their vast training data and sophisticated architectures to generate contextually appropriate responses, which can sometimes create the illusion of understanding.\n",
    "\n",
    " LLMs are essentially very good at pattern recognition and can generate coherent and contextually relevant responses based on the patterns they have observed in their training data. They can capture complex relationships between words, phrases, and concepts, and they can often generate plausible answers or complete sentences that appear to demonstrate understanding.\n",
    "\n",
    "However, their \"knowledge\" is limited to the statistical relationships between words and phrases in their training data, and they lack true reasoning or common sense abilities. They do not have the capacity to understand the underlying meaning or intent of a sentence or concept in the same way a human would.\n",
    "\n",
    "It is essential to recognize the limitations of these models and to be cautious when interpreting their responses, as they can sometimes generate plausible-sounding but incorrect or nonsensical answers. Despite their impressive performance, they still have a long way to go before they can truly understand language and concepts like humans do.\n",
    "\n",
    "I am excited to see progress in the near future for the following research topics:\n",
    "1. Explainability and interpretability: Developing models that can provide insight into their internal workings and decision-making processes, making it easier to understand and trust their outputs.\n",
    "2. Common sense reasoning: Integrating common sense knowledge and reasoning abilities into NLP models to improve their understanding of context, implicit knowledge, and unspoken assumptions.\n",
    "3. Transfer learning and few-shot learning: Improving the ability of NLP models to learn from limited data or adapt to new tasks with minimal fine-tuning or retraining, making them more efficient and versatile.\n",
    "4. Multimodal learning: Combining language understanding with other modalities, such as vision or audio, to develop models that can process and interpret information from multiple sources simultaneously, similar to human cognition.\n",
    "5. Multilingual and cross-lingual models: Building models that can understand and generate text in multiple languages, and transfer knowledge learned in one language to another, increasing their applicability and usefulness across different languages and cultures.\n",
    "6. Bias and fairness: Identifying and addressing bias in NLP models, ensuring that they treat different groups and individuals fairly and do not perpetuate harmful stereotypes or discrimination.\n",
    "7. Safety and robustness: Developing methods to make NLP models more resistant to adversarial attacks and ensuring that they can handle unexpected inputs gracefully and without generating harmful or offensive outputs.\n",
    "8. Resource-efficient models: Reducing the computational requirements and environmental impact of training and deploying large NLP models by developing more efficient architectures, algorithms, and training techniques.\n",
    "\n",
    "With breakthroughs in the above research areas, we will see AI even more useful in tackling the deeper questions of NLP."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
