{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Cleaning Tweets\n",
    "\n",
    "This notebook will explore the tweet posts in detail and will explore functions to clean the data. The first section will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "in_tweet_file = 'zm-tweet-2020-9-11-to-2020-9-13.csv'\n",
    "tweet_df  = pd.read_csv(in_tweet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>463</td>\n",
       "      <td>2020-09-11 00:02:59+00:00</td>\n",
       "      <td>https://t.co/9VjKMnpm7n End-of-Day Sort for 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>462</td>\n",
       "      <td>2020-09-11 00:07:36+00:00</td>\n",
       "      <td>@cadeinvests So true! Saw that play out recent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>461</td>\n",
       "      <td>2020-09-11 00:12:33+00:00</td>\n",
       "      <td>5 Stocks in ETF Witnessing a Spike on the #Nas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>460</td>\n",
       "      <td>2020-09-11 00:15:58+00:00</td>\n",
       "      <td>‰ªäÂæå„ÅØ $ZM $CRWD $FB „ÇíÈï∑Êúü„Éõ„Éº„É´„Éâ„ÄÅ $FSLY „Çí„Çπ„Ç§„É≥„Ç∞„Åó„Å§„Å§‰∏ÄÂÆöÊï∞„Éõ„Éº...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>459</td>\n",
       "      <td>2020-09-11 00:16:27+00:00</td>\n",
       "      <td>$BYND $VIAC $SPY $PLAY were the plays today!\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-09-12 23:16:44+00:00</td>\n",
       "      <td>@marketmusician @BahamaBen9 $WORK is not $ZM -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-09-12 23:20:15+00:00</td>\n",
       "      <td>@marketmusician @BahamaBen9 $WORK is a land an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-09-12 23:22:25+00:00</td>\n",
       "      <td>@marketmusician @BahamaBen9 Finally, the $WORK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-09-12 23:22:59+00:00</td>\n",
       "      <td>@JaxxTx @JonahLupton $SE 10.46% $LVGO 6% $PINS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-09-12 23:42:31+00:00</td>\n",
       "      <td>$HTSC, They have a nickel property and are goi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>464 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                       date  \\\n",
       "0      463  2020-09-11 00:02:59+00:00   \n",
       "1      462  2020-09-11 00:07:36+00:00   \n",
       "2      461  2020-09-11 00:12:33+00:00   \n",
       "3      460  2020-09-11 00:15:58+00:00   \n",
       "4      459  2020-09-11 00:16:27+00:00   \n",
       "..     ...                        ...   \n",
       "459      4  2020-09-12 23:16:44+00:00   \n",
       "460      3  2020-09-12 23:20:15+00:00   \n",
       "461      2  2020-09-12 23:22:25+00:00   \n",
       "462      1  2020-09-12 23:22:59+00:00   \n",
       "463      0  2020-09-12 23:42:31+00:00   \n",
       "\n",
       "                                               content  \n",
       "0    https://t.co/9VjKMnpm7n End-of-Day Sort for 20...  \n",
       "1    @cadeinvests So true! Saw that play out recent...  \n",
       "2    5 Stocks in ETF Witnessing a Spike on the #Nas...  \n",
       "3    ‰ªäÂæå„ÅØ $ZM $CRWD $FB „ÇíÈï∑Êúü„Éõ„Éº„É´„Éâ„ÄÅ $FSLY „Çí„Çπ„Ç§„É≥„Ç∞„Åó„Å§„Å§‰∏ÄÂÆöÊï∞„Éõ„Éº...  \n",
       "4    $BYND $VIAC $SPY $PLAY were the plays today!\\n...  \n",
       "..                                                 ...  \n",
       "459  @marketmusician @BahamaBen9 $WORK is not $ZM -...  \n",
       "460  @marketmusician @BahamaBen9 $WORK is a land an...  \n",
       "461  @marketmusician @BahamaBen9 Finally, the $WORK...  \n",
       "462  @JaxxTx @JonahLupton $SE 10.46% $LVGO 6% $PINS...  \n",
       "463  $HTSC, They have a nickel property and are goi...  \n",
       "\n",
       "[464 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tweet_df[['date', 'content']].tail()\n",
    "tweet_df_date_sorted = tweet_df[['date', 'content']].sort_values('date').reset_index()\n",
    "\n",
    "tweet_df_date_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464\n"
     ]
    }
   ],
   "source": [
    "tweet_content_list = []\n",
    "\n",
    "for i, row in tweet_df_date_sorted.iterrows():\n",
    "    tweet_content_list.append([row['date'], row['content']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(tweet_content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--tweet--\n",
      "2020-09-12 13:20:19+00:00\n",
      "remove $ZM from my list\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 13:29:57+00:00\n",
      "$ZM This stock is 20% off its high &amp; found support at its 10ema. The reason software had such a big run is because that is where the growth is. There are not many companies growing like ZM. Who knows if this one will continue to hold up? Watch the key technical levels for clues https://t.co/iYjzsxH5xp\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 13:51:25+00:00\n",
      "$ZM Weekly. Inside wk. Big fat range here, so $ZM could just chop sideways for a bit - the range is big, tho, so that makes it interesting to me b/c I can play in a big range. Anyway, will be watching next wk to see if it can go inside wk + up....or trapped sideways... https://t.co/SWiFWyLO1j\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 14:15:38+00:00\n",
      "From ATH \n",
      "\n",
      "$SPX -7%\n",
      "$QQQ -11%\n",
      "\n",
      "$V -8\n",
      "$BABA $WMT -9\n",
      "$MA -10\n",
      "$ATVI $NOW -11\n",
      "$AMZN $FB $GOOGL $MSFT -12\n",
      "$PYPL -13\n",
      "$CRM $EA $PTON -15\n",
      "$NFLX $ROKU -16\n",
      "$NVDA -17\n",
      "$AAPL $AMD $TTD -19\n",
      "$SHOP $SQ $ZM -20\n",
      "$AMAT $KLAC -21\n",
      "$TWLO $ZS -22\n",
      "$LRCX -24\n",
      "$TSLA -26\n",
      "$DOCU -32\n",
      "$FSLY -36\n",
      "$BIGC -49\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 14:30:20+00:00\n",
      "#AutomatedTrading #AlgoTrading #MachineLearning\n",
      "$ZM $DOCU $CRWD $M $CTLT $HOME $CPB $AVGO $SWBI $CLDR $HRB $MOMO $GSX $ZUO $MDB $MIK $SCSC $VRA $SPWH $WSG \n",
      "#stocks #DayTrading #SwingTrading\n",
      "\n",
      "üôã  15% off Monthly/Yearly with Coupon Code: MYDISCOUNT15\n",
      "https://t.co/egbvLSf50A\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 14:35:17+00:00\n",
      "$SPY $QQQ $IWM now at or below the 50MAs on daily and weekly charts.\n",
      "Most tech leaders ended the week soft...pulling below 9/20MAs and hovering at or below the 50. \n",
      "\n",
      "$ZM $RH $PTON $FVRR $PINS $CRWD $APPS $QCOM (closed above Friday's open!) $CMG are still trading around 9/20DMAs https://t.co/E9Elaa3I3s\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 14:38:15+00:00\n",
      "Umm NO.  The Green New Deal isn‚Äôt needed, #COVID19 has done all the heavy lifting to reduce GHGs.  \n",
      "\n",
      "Zoom has reduced more carbon in the atmosphere than Telsa ever has or ever will - WITHOUT government handouts.  $ZM $TSLA https://t.co/5ZruaNqCUG\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 14:38:57+00:00\n",
      "let's say within 5 yrs i want to assemble a trading team...there are just so many stocks to cover.  for example, I just couldn't focus on $CHWY when i had just executed $PTON n then followed by $ORCL.   That's not counting the $ZM $CRWD $QQQ easy short setups..a ton of opptys\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 14:55:22+00:00\n",
      "@charliebilello $ZM Zoom Zoom Zoom and it‚Äôs not even close!\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 15:10:07+00:00\n",
      "„Ç∏„É†„Éª„ÇØ„É¨„Ç§„Éû„ÉºÁ≠â„ÇÇÂà©Á¢∫„ÇíË®Ä„Å£„Å¶„ÅÑ„ÇãÁÇπ„ÄÅÂ∞ë„Å™„Åè„Å®„ÇÇ„Éè„Ç§„ÉÜ„ÇØÊ†™„Å´„Å§„ÅÑ„Å¶Â∞ë„ÅóËÄÉ„Åà„ÇãÊôÇÊúü„Åß„ÅØ„ÅÇ„Çä„Åù„ÅÜ„ÄÇ„Çø„ÉÄÊ†™„Å´„Åó„Åü $zm $okta $crwd „ÅØ„Å®„ÇÇ„Åã„Åè‰ªñ„ÅÆÊ†™„ÅØÂ∞ë„ÅóÊï¥ÁêÜ„Åô„Çã„Åì„Å®„ÇÇÊ§úË®é„ÄÇ\n",
      "$u „ÅÆipo„ÇÇÈëë„Åø„ÄÅ‰∏ÄÈÉ®„Å†„ÅëÂ£≤„Çã„Å®„ÅÑ„ÅÜ„ÅÆ„ÇÇ„ÅÇ„Çä„Åã„ÇÇ„Åó„Çå„Å™„ÅÑ„ÄÇ\n",
      "#„Ç¢„É°„É™„Ç´Ê†™\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 15:18:38+00:00\n",
      "All these companies profiting huge of this pandemic\n",
      "\n",
      "Systemic changes have caused Zoom $ZM Amazon $AMZN Netflix $NFLX and Peloton $PTON to be the new everyday products people use\n",
      "\n",
      "With that in mind my vote would be for $PTON Or $ZM as they really came out of no where in 2020 https://t.co/LJ0F3QxGgn\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 15:20:07+00:00\n",
      "$ZM Looks like a base is forming at $375.00, Folks Zoom is not going anywhere, Working from home will be the new norm for most as it saves companies a lot of money and makes things easier plus the news stations are all using Z... https://t.co/51VdMrHQYL\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 15:47:13+00:00\n",
      "@baldassarifr I think there are examples of both. Something like innovating with existing tech and solved problems like $ZM has a boost from having their core tech teams on chinese salaries\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 15:59:09+00:00\n",
      "I like $ZM buying $WORK. Creates a bigger feature set and becomes a possible work place must have. https://t.co/aZ7xsTh52v\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 16:05:01+00:00\n",
      "#AutomatedTrading #AlgoTrading #MachineLearning\n",
      "$ZM $DOCU $CRWD $M $CTLT $HOME $CPB $AVGO $SWBI $CLDR $HRB $MOMO $DHT $NNOX $TECK $DPHC $BTU $GSX $AYTU $TRNE \n",
      "#stocks #DayTrading #SwingTrading\n",
      "\n",
      "üôã  15% off Monthly/Yearly with Coupon Code: MYDISCOUNT15\n",
      "https://t.co/egbvLSf50A\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 16:08:31+00:00\n",
      "$ZM https://t.co/90JNjLyukB\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 16:16:43+00:00\n",
      "Markets are screwed now for real. \n",
      "\n",
      "$SPY $NDX $QQQ $DJIA $IWM $VIX $VXX $DIA $DJI $SLV $GLD $GDX $SIL $UUP $XLK $IYW $AMZN $MSFT $TSLA $FB $GOOG $SHOP $SPOT $AAPL $W $NVDA $ZM https://t.co/NhqaBzyJTd\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 16:21:38+00:00\n",
      "@iamjohn_c with the Saturday morning lessons!  Thank you sir, looking forward to Sunday evening and next Saturday's $ZM course https://t.co/t74My2knhS\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 16:26:12+00:00\n",
      "@Matt_Cochrane7 In 2nd case it should be the way around $Work to buy $ZM\n",
      "-- ## -- \n",
      "\n",
      "--tweet--\n",
      "2020-09-12 16:36:01+00:00\n",
      "@Matt_Cochrane7 $ZM and $CHGG merge\n",
      "-- ## -- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_num = 400\n",
    "\n",
    "for i in tweet_content_list[start_num:start_num+20]:\n",
    "    print ('--tweet--')\n",
    "    print (i[0])\n",
    "    print (i[1])\n",
    "    print ('-- ## -- \\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to Clean Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Remove all tweets that are are not English Language\n",
    "\n",
    "The scraped tweets have a value 'lang' which is the language of the tweet. All tweets that are not of 'lang' = 'en' will be removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Removing emojis\n",
    "\n",
    "From great article about sentiment analysis: https://heartbeat.comet.ml/twitter-sentiment-analysis-part-1-6063442c06f3\n",
    "\n",
    "The author used the following functions to remove emojis and non-english characters\n",
    "\n",
    "I have employed her functions in my project to remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet with emojis: \t\t\tA Couple Morning Plays With One        üî• üìà Running Over 60% üìà üî•\n",
      "Tweet after using remove_emoji function: \tA Couple Morning Plays With One          Running Over 60%  \n"
     ]
    }
   ],
   "source": [
    "# examples of using the above functions\n",
    "\n",
    "tweet_sample_w_emojis = '''A Couple Morning Plays With One        üî• üìà Running Over 60% üìà üî•'''\n",
    "\n",
    "print(f'Original tweet with emojis: \\t\\t\\t{tweet_sample_w_emojis}')\n",
    "print(f'Tweet after using remove_emoji function: \\t{remove_emoji(tweet_sample_w_emojis)}')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove Hyperlinks, Twitter marks and styles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tweet: \n",
      "Stats for the day have arrived. 1 new follower and NO unfollowers :) via http://t.co/0s8GQYOeus.\n",
      "\n",
      "After removing old style tweet, hyperlinks and # sign\n",
      "Stats for the day have arrived.  new follower and NO unfollowers :) via \n"
     ]
    }
   ],
   "source": [
    "tweet = 'Stats for the day have arrived. 1 new follower and NO unfollowers :) via http://t.co/0s8GQYOeus.'\n",
    "\n",
    "\n",
    "print('Original Tweet: ')\n",
    "print(tweet)\n",
    "\n",
    "# it will remove the old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "# it will remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "\n",
    "# it will remove hashtags. We have to be careful here not to remove \n",
    "# the whole hashtag because text of hashtags contains huge information. \n",
    "# only removing the hash # sign from the word\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "\n",
    "# it will remove single numeric terms in the tweet. \n",
    "tweet2 = re.sub(r'[0-9]', '', tweet2)\n",
    "print('\\nAfter removing old style tweet, hyperlinks and # sign')\n",
    "print(tweet2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Remove Stop Words, Punctuations and Stemming\n",
    "\n",
    "Stop words and Punctuations are to be removed for the data set for the SVM model. I believe that keeping stop words and punctuations for the BERT model will be more beneficial. If there is time, I will test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk                             \n",
    "from nltk.corpus import twitter_samples   \n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('twitter_samples')\n",
    "\n",
    "import re                                  \n",
    "import string                             \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regarding Stop Words\n",
    "\n",
    "- We can see that the stop words list above contains some words that could be important in some contexts. These could be words like i, not, between, because, won, against. I will customize the stop words list for the SVM model as it doesn't take the sentence into context but just individual words.  For the Roberta model, we will keep the stop words as is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Tokenizing: \n",
      "Stats for the day have arrived.  new follower and NO unfollowers :) via \n",
      "\n",
      "Tokenized string:\n",
      "['stats', 'for', 'the', 'day', 'have', 'arrived', '.', 'new', 'follower', 'and', 'no', 'unfollowers', ':)', 'via']\n"
     ]
    }
   ],
   "source": [
    "print('Before Tokenizing: ')\n",
    "print(tweet2)\n",
    "\n",
    "# instantiate the tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, \n",
    "                           strip_handles=True,\n",
    "                           reduce_len=True)\n",
    "\n",
    "# tokenize the tweets\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "print('\\nTokenized string:')\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization\n",
      "['stats', 'for', 'the', 'day', 'have', 'arrived', '.', 'new', 'follower', 'and', 'no', 'unfollowers', ':)', 'via']\n",
      "\n",
      "\n",
      "After removing stop words and punctuation:\n",
      "['stats', 'day', 'arrived', 'new', 'follower', 'unfollowers', ':)', 'via']\n"
     ]
    }
   ],
   "source": [
    "print('Before tokenization')\n",
    "print(tweet_tokens)\n",
    "\n",
    "\n",
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stopwords_english and  # remove stopwords\n",
    "        word not in string.punctuation):  # remove punctuation\n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print('\\n\\nAfter removing stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words\n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english') \n",
    "\n",
    "print('Stop words\\n')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before tokenization\n",
      "['stats', 'for', 'the', 'day', 'have', 'arrived', '.', 'new', 'follower', 'and', 'no', 'unfollowers', ':)', 'via']\n",
      "\n",
      "\n",
      "After removing stop words and punctuation:\n",
      "['stats', 'day', 'arrived', 'new', 'follower', 'unfollowers', ':)', 'via']\n"
     ]
    }
   ],
   "source": [
    "print('Before tokenization')\n",
    "print(tweet_tokens)\n",
    "\n",
    "\n",
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens: # Go through every word in your tokens list\n",
    "    if (word not in stopwords_english and  # remove stopwords\n",
    "        word not in string.punctuation):  # remove punctuation\n",
    "        tweets_clean.append(word)\n",
    "\n",
    "print('\\n\\nAfter removing stop words and punctuation:')\n",
    "print(tweets_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n",
    "\n",
    "Consider the words:\n",
    "\n",
    "- learn\n",
    "- learning\n",
    "- learned\n",
    "- learnt\n",
    "\n",
    "All these words are stemmed from its common root learn. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, happi and sunni. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n",
    "\n",
    "- happy\n",
    "- happiness\n",
    "- happier\n",
    "\n",
    "We can see that the prefix happi is more commonly used. We cannot choose happ because it is the stem of unrelated words like happen.\n",
    "\n",
    "NLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let's see how we can use it in the cell below.\n",
    "\n",
    "\n",
    "\n",
    "Please note that Stemming will be used for the SVM model. As I feel that having the full word for Roberta will yeild a more accurante sentiment score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words after stemming: \n",
      "['stat', 'day', 'arriv', 'new', 'follow', 'unfollow', ':)', 'via']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate stemming class\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Create an empty list to store the stems\n",
    "tweets_stem = [] \n",
    "\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)  # stemming word\n",
    "    tweets_stem.append(stem_word)  # append to the list\n",
    "\n",
    "print('Words after stemming: ')\n",
    "print(tweets_stem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fd53b764256a4b472faf6c58b857470c56c5be1791fa6563b7af5cb6212701e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
