{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this is an older code without reporting cuda and batch number\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a list of 17,000 sample texts (replace this with your actual data)\n",
    "texts = [\"Sample text {}\".format(i) for i in range(17000)]\n",
    "\n",
    "# Set the batch size (you may need to adjust this based on your GPU memory)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to classify sentiment in a batch of texts\n",
    "def classify_sentiment(texts_batch):\n",
    "    encoded_input = tokenizer(texts_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {key: value.to(\"cuda\" if torch.cuda.is_available() else \"cpu\") for key, value in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded_input).logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    labels = [\"NEGATIVE\" if p < 0.5 else \"POSITIVE\" for p in probabilities[:, 1]]\n",
    "    return labels\n",
    "\n",
    "# Process the texts in batches\n",
    "sentiments = []\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    texts_batch = texts[i:i + batch_size]\n",
    "    sentiments_batch = classify_sentiment(texts_batch)\n",
    "    sentiments.extend(sentiments_batch)\n",
    "\n",
    "# Print the first 10 sentiment labels\n",
    "print(sentiments[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/532 completed. Batches left: 531\n",
      "Batch 2/532 completed. Batches left: 530\n",
      "Batch 3/532 completed. Batches left: 529\n",
      "Batch 4/532 completed. Batches left: 528\n",
      "Batch 5/532 completed. Batches left: 527\n",
      "Batch 6/532 completed. Batches left: 526\n",
      "Batch 7/532 completed. Batches left: 525\n",
      "Batch 8/532 completed. Batches left: 524\n",
      "Batch 9/532 completed. Batches left: 523\n",
      "Batch 10/532 completed. Batches left: 522\n",
      "Batch 11/532 completed. Batches left: 521\n",
      "Batch 12/532 completed. Batches left: 520\n",
      "Batch 13/532 completed. Batches left: 519\n",
      "Batch 14/532 completed. Batches left: 518\n",
      "Batch 15/532 completed. Batches left: 517\n",
      "Batch 16/532 completed. Batches left: 516\n",
      "Batch 17/532 completed. Batches left: 515\n",
      "Batch 18/532 completed. Batches left: 514\n",
      "Batch 19/532 completed. Batches left: 513\n",
      "Batch 20/532 completed. Batches left: 512\n",
      "Batch 21/532 completed. Batches left: 511\n",
      "Batch 22/532 completed. Batches left: 510\n",
      "Batch 23/532 completed. Batches left: 509\n",
      "Batch 24/532 completed. Batches left: 508\n",
      "Batch 25/532 completed. Batches left: 507\n",
      "Batch 26/532 completed. Batches left: 506\n",
      "Batch 27/532 completed. Batches left: 505\n",
      "Batch 28/532 completed. Batches left: 504\n",
      "Batch 29/532 completed. Batches left: 503\n",
      "Batch 30/532 completed. Batches left: 502\n",
      "Batch 31/532 completed. Batches left: 501\n",
      "Batch 32/532 completed. Batches left: 500\n",
      "Batch 33/532 completed. Batches left: 499\n",
      "Batch 34/532 completed. Batches left: 498\n",
      "Batch 35/532 completed. Batches left: 497\n",
      "Batch 36/532 completed. Batches left: 496\n",
      "Batch 37/532 completed. Batches left: 495\n",
      "Batch 38/532 completed. Batches left: 494\n",
      "Batch 39/532 completed. Batches left: 493\n",
      "Batch 40/532 completed. Batches left: 492\n",
      "Batch 41/532 completed. Batches left: 491\n",
      "Batch 42/532 completed. Batches left: 490\n",
      "Batch 43/532 completed. Batches left: 489\n",
      "Batch 44/532 completed. Batches left: 488\n",
      "Batch 45/532 completed. Batches left: 487\n",
      "Batch 46/532 completed. Batches left: 486\n",
      "Batch 47/532 completed. Batches left: 485\n",
      "Batch 48/532 completed. Batches left: 484\n",
      "Batch 49/532 completed. Batches left: 483\n",
      "Batch 50/532 completed. Batches left: 482\n",
      "Batch 51/532 completed. Batches left: 481\n",
      "Batch 52/532 completed. Batches left: 480\n",
      "Batch 53/532 completed. Batches left: 479\n",
      "Batch 54/532 completed. Batches left: 478\n",
      "Batch 55/532 completed. Batches left: 477\n",
      "Batch 56/532 completed. Batches left: 476\n",
      "Batch 57/532 completed. Batches left: 475\n",
      "Batch 58/532 completed. Batches left: 474\n",
      "Batch 59/532 completed. Batches left: 473\n",
      "Batch 60/532 completed. Batches left: 472\n",
      "Batch 61/532 completed. Batches left: 471\n",
      "Batch 62/532 completed. Batches left: 470\n",
      "Batch 63/532 completed. Batches left: 469\n",
      "Batch 64/532 completed. Batches left: 468\n",
      "Batch 65/532 completed. Batches left: 467\n",
      "Batch 66/532 completed. Batches left: 466\n",
      "Batch 67/532 completed. Batches left: 465\n",
      "Batch 68/532 completed. Batches left: 464\n",
      "Batch 69/532 completed. Batches left: 463\n",
      "Batch 70/532 completed. Batches left: 462\n",
      "Batch 71/532 completed. Batches left: 461\n",
      "Batch 72/532 completed. Batches left: 460\n",
      "Batch 73/532 completed. Batches left: 459\n",
      "Batch 74/532 completed. Batches left: 458\n",
      "Batch 75/532 completed. Batches left: 457\n",
      "Batch 76/532 completed. Batches left: 456\n",
      "Batch 77/532 completed. Batches left: 455\n",
      "Batch 78/532 completed. Batches left: 454\n",
      "Batch 79/532 completed. Batches left: 453\n",
      "Batch 80/532 completed. Batches left: 452\n",
      "Batch 81/532 completed. Batches left: 451\n",
      "Batch 82/532 completed. Batches left: 450\n",
      "Batch 83/532 completed. Batches left: 449\n",
      "Batch 84/532 completed. Batches left: 448\n",
      "Batch 85/532 completed. Batches left: 447\n",
      "Batch 86/532 completed. Batches left: 446\n",
      "Batch 87/532 completed. Batches left: 445\n",
      "Batch 88/532 completed. Batches left: 444\n",
      "Batch 89/532 completed. Batches left: 443\n",
      "Batch 90/532 completed. Batches left: 442\n",
      "Batch 91/532 completed. Batches left: 441\n",
      "Batch 92/532 completed. Batches left: 440\n",
      "Batch 93/532 completed. Batches left: 439\n",
      "Batch 94/532 completed. Batches left: 438\n",
      "Batch 95/532 completed. Batches left: 437\n",
      "Batch 96/532 completed. Batches left: 436\n",
      "Batch 97/532 completed. Batches left: 435\n",
      "Batch 98/532 completed. Batches left: 434\n",
      "Batch 99/532 completed. Batches left: 433\n",
      "Batch 100/532 completed. Batches left: 432\n",
      "Batch 101/532 completed. Batches left: 431\n",
      "Batch 102/532 completed. Batches left: 430\n",
      "Batch 103/532 completed. Batches left: 429\n",
      "Batch 104/532 completed. Batches left: 428\n",
      "Batch 105/532 completed. Batches left: 427\n",
      "Batch 106/532 completed. Batches left: 426\n",
      "Batch 107/532 completed. Batches left: 425\n",
      "Batch 108/532 completed. Batches left: 424\n",
      "Batch 109/532 completed. Batches left: 423\n",
      "Batch 110/532 completed. Batches left: 422\n",
      "Batch 111/532 completed. Batches left: 421\n",
      "Batch 112/532 completed. Batches left: 420\n",
      "Batch 113/532 completed. Batches left: 419\n",
      "Batch 114/532 completed. Batches left: 418\n",
      "Batch 115/532 completed. Batches left: 417\n",
      "Batch 116/532 completed. Batches left: 416\n",
      "Batch 117/532 completed. Batches left: 415\n",
      "Batch 118/532 completed. Batches left: 414\n",
      "Batch 119/532 completed. Batches left: 413\n",
      "Batch 120/532 completed. Batches left: 412\n",
      "Batch 121/532 completed. Batches left: 411\n",
      "Batch 122/532 completed. Batches left: 410\n",
      "Batch 123/532 completed. Batches left: 409\n",
      "Batch 124/532 completed. Batches left: 408\n",
      "Batch 125/532 completed. Batches left: 407\n",
      "Batch 126/532 completed. Batches left: 406\n",
      "Batch 127/532 completed. Batches left: 405\n",
      "Batch 128/532 completed. Batches left: 404\n",
      "Batch 129/532 completed. Batches left: 403\n",
      "Batch 130/532 completed. Batches left: 402\n",
      "Batch 131/532 completed. Batches left: 401\n",
      "Batch 132/532 completed. Batches left: 400\n",
      "Batch 133/532 completed. Batches left: 399\n",
      "Batch 134/532 completed. Batches left: 398\n",
      "Batch 135/532 completed. Batches left: 397\n",
      "Batch 136/532 completed. Batches left: 396\n",
      "Batch 137/532 completed. Batches left: 395\n",
      "Batch 138/532 completed. Batches left: 394\n",
      "Batch 139/532 completed. Batches left: 393\n",
      "Batch 140/532 completed. Batches left: 392\n",
      "Batch 141/532 completed. Batches left: 391\n",
      "Batch 142/532 completed. Batches left: 390\n",
      "Batch 143/532 completed. Batches left: 389\n",
      "Batch 144/532 completed. Batches left: 388\n",
      "Batch 145/532 completed. Batches left: 387\n",
      "Batch 146/532 completed. Batches left: 386\n",
      "Batch 147/532 completed. Batches left: 385\n",
      "Batch 148/532 completed. Batches left: 384\n",
      "Batch 149/532 completed. Batches left: 383\n",
      "Batch 150/532 completed. Batches left: 382\n",
      "Batch 151/532 completed. Batches left: 381\n",
      "Batch 152/532 completed. Batches left: 380\n",
      "Batch 153/532 completed. Batches left: 379\n",
      "Batch 154/532 completed. Batches left: 378\n",
      "Batch 155/532 completed. Batches left: 377\n",
      "Batch 156/532 completed. Batches left: 376\n",
      "Batch 157/532 completed. Batches left: 375\n",
      "Batch 158/532 completed. Batches left: 374\n",
      "Batch 159/532 completed. Batches left: 373\n",
      "Batch 160/532 completed. Batches left: 372\n",
      "Batch 161/532 completed. Batches left: 371\n",
      "Batch 162/532 completed. Batches left: 370\n",
      "Batch 163/532 completed. Batches left: 369\n",
      "Batch 164/532 completed. Batches left: 368\n",
      "Batch 165/532 completed. Batches left: 367\n",
      "Batch 166/532 completed. Batches left: 366\n",
      "Batch 167/532 completed. Batches left: 365\n",
      "Batch 168/532 completed. Batches left: 364\n",
      "Batch 169/532 completed. Batches left: 363\n",
      "Batch 170/532 completed. Batches left: 362\n",
      "Batch 171/532 completed. Batches left: 361\n",
      "Batch 172/532 completed. Batches left: 360\n",
      "Batch 173/532 completed. Batches left: 359\n",
      "Batch 174/532 completed. Batches left: 358\n",
      "Batch 175/532 completed. Batches left: 357\n",
      "Batch 176/532 completed. Batches left: 356\n",
      "Batch 177/532 completed. Batches left: 355\n",
      "Batch 178/532 completed. Batches left: 354\n",
      "Batch 179/532 completed. Batches left: 353\n",
      "Batch 180/532 completed. Batches left: 352\n",
      "Batch 181/532 completed. Batches left: 351\n",
      "Batch 182/532 completed. Batches left: 350\n",
      "Batch 183/532 completed. Batches left: 349\n",
      "Batch 184/532 completed. Batches left: 348\n",
      "Batch 185/532 completed. Batches left: 347\n",
      "Batch 186/532 completed. Batches left: 346\n",
      "Batch 187/532 completed. Batches left: 345\n",
      "Batch 188/532 completed. Batches left: 344\n",
      "Batch 189/532 completed. Batches left: 343\n",
      "Batch 190/532 completed. Batches left: 342\n",
      "Batch 191/532 completed. Batches left: 341\n",
      "Batch 192/532 completed. Batches left: 340\n",
      "Batch 193/532 completed. Batches left: 339\n",
      "Batch 194/532 completed. Batches left: 338\n",
      "Batch 195/532 completed. Batches left: 337\n",
      "Batch 196/532 completed. Batches left: 336\n",
      "Batch 197/532 completed. Batches left: 335\n",
      "Batch 198/532 completed. Batches left: 334\n",
      "Batch 199/532 completed. Batches left: 333\n",
      "Batch 200/532 completed. Batches left: 332\n",
      "Batch 201/532 completed. Batches left: 331\n",
      "Batch 202/532 completed. Batches left: 330\n",
      "Batch 203/532 completed. Batches left: 329\n",
      "Batch 204/532 completed. Batches left: 328\n",
      "Batch 205/532 completed. Batches left: 327\n",
      "Batch 206/532 completed. Batches left: 326\n",
      "Batch 207/532 completed. Batches left: 325\n",
      "Batch 208/532 completed. Batches left: 324\n",
      "Batch 209/532 completed. Batches left: 323\n",
      "Batch 210/532 completed. Batches left: 322\n",
      "Batch 211/532 completed. Batches left: 321\n",
      "Batch 212/532 completed. Batches left: 320\n",
      "Batch 213/532 completed. Batches left: 319\n",
      "Batch 214/532 completed. Batches left: 318\n",
      "Batch 215/532 completed. Batches left: 317\n",
      "Batch 216/532 completed. Batches left: 316\n",
      "Batch 217/532 completed. Batches left: 315\n",
      "Batch 218/532 completed. Batches left: 314\n",
      "Batch 219/532 completed. Batches left: 313\n",
      "Batch 220/532 completed. Batches left: 312\n",
      "Batch 221/532 completed. Batches left: 311\n",
      "Batch 222/532 completed. Batches left: 310\n",
      "Batch 223/532 completed. Batches left: 309\n",
      "Batch 224/532 completed. Batches left: 308\n",
      "Batch 225/532 completed. Batches left: 307\n",
      "Batch 226/532 completed. Batches left: 306\n",
      "Batch 227/532 completed. Batches left: 305\n",
      "Batch 228/532 completed. Batches left: 304\n",
      "Batch 229/532 completed. Batches left: 303\n",
      "Batch 230/532 completed. Batches left: 302\n",
      "Batch 231/532 completed. Batches left: 301\n",
      "Batch 232/532 completed. Batches left: 300\n",
      "Batch 233/532 completed. Batches left: 299\n",
      "Batch 234/532 completed. Batches left: 298\n",
      "Batch 235/532 completed. Batches left: 297\n",
      "Batch 236/532 completed. Batches left: 296\n",
      "Batch 237/532 completed. Batches left: 295\n",
      "Batch 238/532 completed. Batches left: 294\n",
      "Batch 239/532 completed. Batches left: 293\n",
      "Batch 240/532 completed. Batches left: 292\n",
      "Batch 241/532 completed. Batches left: 291\n",
      "Batch 242/532 completed. Batches left: 290\n",
      "Batch 243/532 completed. Batches left: 289\n",
      "Batch 244/532 completed. Batches left: 288\n",
      "Batch 245/532 completed. Batches left: 287\n",
      "Batch 246/532 completed. Batches left: 286\n",
      "Batch 247/532 completed. Batches left: 285\n",
      "Batch 248/532 completed. Batches left: 284\n",
      "Batch 249/532 completed. Batches left: 283\n",
      "Batch 250/532 completed. Batches left: 282\n",
      "Batch 251/532 completed. Batches left: 281\n",
      "Batch 252/532 completed. Batches left: 280\n",
      "Batch 253/532 completed. Batches left: 279\n",
      "Batch 254/532 completed. Batches left: 278\n",
      "Batch 255/532 completed. Batches left: 277\n",
      "Batch 256/532 completed. Batches left: 276\n",
      "Batch 257/532 completed. Batches left: 275\n",
      "Batch 258/532 completed. Batches left: 274\n",
      "Batch 259/532 completed. Batches left: 273\n",
      "Batch 260/532 completed. Batches left: 272\n",
      "Batch 261/532 completed. Batches left: 271\n",
      "Batch 262/532 completed. Batches left: 270\n",
      "Batch 263/532 completed. Batches left: 269\n",
      "Batch 264/532 completed. Batches left: 268\n",
      "Batch 265/532 completed. Batches left: 267\n",
      "Batch 266/532 completed. Batches left: 266\n",
      "Batch 267/532 completed. Batches left: 265\n",
      "Batch 268/532 completed. Batches left: 264\n",
      "Batch 269/532 completed. Batches left: 263\n",
      "Batch 270/532 completed. Batches left: 262\n",
      "Batch 271/532 completed. Batches left: 261\n",
      "Batch 272/532 completed. Batches left: 260\n",
      "Batch 273/532 completed. Batches left: 259\n",
      "Batch 274/532 completed. Batches left: 258\n",
      "Batch 275/532 completed. Batches left: 257\n",
      "Batch 276/532 completed. Batches left: 256\n",
      "Batch 277/532 completed. Batches left: 255\n",
      "Batch 278/532 completed. Batches left: 254\n",
      "Batch 279/532 completed. Batches left: 253\n",
      "Batch 280/532 completed. Batches left: 252\n",
      "Batch 281/532 completed. Batches left: 251\n",
      "Batch 282/532 completed. Batches left: 250\n",
      "Batch 283/532 completed. Batches left: 249\n",
      "Batch 284/532 completed. Batches left: 248\n",
      "Batch 285/532 completed. Batches left: 247\n",
      "Batch 286/532 completed. Batches left: 246\n",
      "Batch 287/532 completed. Batches left: 245\n",
      "Batch 288/532 completed. Batches left: 244\n",
      "Batch 289/532 completed. Batches left: 243\n",
      "Batch 290/532 completed. Batches left: 242\n",
      "Batch 291/532 completed. Batches left: 241\n",
      "Batch 292/532 completed. Batches left: 240\n",
      "Batch 293/532 completed. Batches left: 239\n",
      "Batch 294/532 completed. Batches left: 238\n",
      "Batch 295/532 completed. Batches left: 237\n",
      "Batch 296/532 completed. Batches left: 236\n",
      "Batch 297/532 completed. Batches left: 235\n",
      "Batch 298/532 completed. Batches left: 234\n",
      "Batch 299/532 completed. Batches left: 233\n",
      "Batch 300/532 completed. Batches left: 232\n",
      "Batch 301/532 completed. Batches left: 231\n",
      "Batch 302/532 completed. Batches left: 230\n",
      "Batch 303/532 completed. Batches left: 229\n",
      "Batch 304/532 completed. Batches left: 228\n",
      "Batch 305/532 completed. Batches left: 227\n",
      "Batch 306/532 completed. Batches left: 226\n",
      "Batch 307/532 completed. Batches left: 225\n",
      "Batch 308/532 completed. Batches left: 224\n",
      "Batch 309/532 completed. Batches left: 223\n",
      "Batch 310/532 completed. Batches left: 222\n",
      "Batch 311/532 completed. Batches left: 221\n",
      "Batch 312/532 completed. Batches left: 220\n",
      "Batch 313/532 completed. Batches left: 219\n",
      "Batch 314/532 completed. Batches left: 218\n",
      "Batch 315/532 completed. Batches left: 217\n",
      "Batch 316/532 completed. Batches left: 216\n",
      "Batch 317/532 completed. Batches left: 215\n",
      "Batch 318/532 completed. Batches left: 214\n",
      "Batch 319/532 completed. Batches left: 213\n",
      "Batch 320/532 completed. Batches left: 212\n",
      "Batch 321/532 completed. Batches left: 211\n",
      "Batch 322/532 completed. Batches left: 210\n",
      "Batch 323/532 completed. Batches left: 209\n",
      "Batch 324/532 completed. Batches left: 208\n",
      "Batch 325/532 completed. Batches left: 207\n",
      "Batch 326/532 completed. Batches left: 206\n",
      "Batch 327/532 completed. Batches left: 205\n",
      "Batch 328/532 completed. Batches left: 204\n",
      "Batch 329/532 completed. Batches left: 203\n",
      "Batch 330/532 completed. Batches left: 202\n",
      "Batch 331/532 completed. Batches left: 201\n",
      "Batch 332/532 completed. Batches left: 200\n",
      "Batch 333/532 completed. Batches left: 199\n",
      "Batch 334/532 completed. Batches left: 198\n",
      "Batch 335/532 completed. Batches left: 197\n",
      "Batch 336/532 completed. Batches left: 196\n",
      "Batch 337/532 completed. Batches left: 195\n",
      "Batch 338/532 completed. Batches left: 194\n",
      "Batch 339/532 completed. Batches left: 193\n",
      "Batch 340/532 completed. Batches left: 192\n",
      "Batch 341/532 completed. Batches left: 191\n",
      "Batch 342/532 completed. Batches left: 190\n",
      "Batch 343/532 completed. Batches left: 189\n",
      "Batch 344/532 completed. Batches left: 188\n",
      "Batch 345/532 completed. Batches left: 187\n",
      "Batch 346/532 completed. Batches left: 186\n",
      "Batch 347/532 completed. Batches left: 185\n",
      "Batch 348/532 completed. Batches left: 184\n",
      "Batch 349/532 completed. Batches left: 183\n",
      "Batch 350/532 completed. Batches left: 182\n",
      "Batch 351/532 completed. Batches left: 181\n",
      "Batch 352/532 completed. Batches left: 180\n",
      "Batch 353/532 completed. Batches left: 179\n",
      "Batch 354/532 completed. Batches left: 178\n",
      "Batch 355/532 completed. Batches left: 177\n",
      "Batch 356/532 completed. Batches left: 176\n",
      "Batch 357/532 completed. Batches left: 175\n",
      "Batch 358/532 completed. Batches left: 174\n",
      "Batch 359/532 completed. Batches left: 173\n",
      "Batch 360/532 completed. Batches left: 172\n",
      "Batch 361/532 completed. Batches left: 171\n",
      "Batch 362/532 completed. Batches left: 170\n",
      "Batch 363/532 completed. Batches left: 169\n",
      "Batch 364/532 completed. Batches left: 168\n",
      "Batch 365/532 completed. Batches left: 167\n",
      "Batch 366/532 completed. Batches left: 166\n",
      "Batch 367/532 completed. Batches left: 165\n",
      "Batch 368/532 completed. Batches left: 164\n",
      "Batch 369/532 completed. Batches left: 163\n",
      "Batch 370/532 completed. Batches left: 162\n",
      "Batch 371/532 completed. Batches left: 161\n",
      "Batch 372/532 completed. Batches left: 160\n",
      "Batch 373/532 completed. Batches left: 159\n",
      "Batch 374/532 completed. Batches left: 158\n",
      "Batch 375/532 completed. Batches left: 157\n",
      "Batch 376/532 completed. Batches left: 156\n",
      "Batch 377/532 completed. Batches left: 155\n",
      "Batch 378/532 completed. Batches left: 154\n",
      "Batch 379/532 completed. Batches left: 153\n",
      "Batch 380/532 completed. Batches left: 152\n",
      "Batch 381/532 completed. Batches left: 151\n",
      "Batch 382/532 completed. Batches left: 150\n",
      "Batch 383/532 completed. Batches left: 149\n",
      "Batch 384/532 completed. Batches left: 148\n",
      "Batch 385/532 completed. Batches left: 147\n",
      "Batch 386/532 completed. Batches left: 146\n",
      "Batch 387/532 completed. Batches left: 145\n",
      "Batch 388/532 completed. Batches left: 144\n",
      "Batch 389/532 completed. Batches left: 143\n",
      "Batch 390/532 completed. Batches left: 142\n",
      "Batch 391/532 completed. Batches left: 141\n",
      "Batch 392/532 completed. Batches left: 140\n",
      "Batch 393/532 completed. Batches left: 139\n",
      "Batch 394/532 completed. Batches left: 138\n",
      "Batch 395/532 completed. Batches left: 137\n",
      "Batch 396/532 completed. Batches left: 136\n",
      "Batch 397/532 completed. Batches left: 135\n",
      "Batch 398/532 completed. Batches left: 134\n",
      "Batch 399/532 completed. Batches left: 133\n",
      "Batch 400/532 completed. Batches left: 132\n",
      "Batch 401/532 completed. Batches left: 131\n",
      "Batch 402/532 completed. Batches left: 130\n",
      "Batch 403/532 completed. Batches left: 129\n",
      "Batch 404/532 completed. Batches left: 128\n",
      "Batch 405/532 completed. Batches left: 127\n",
      "Batch 406/532 completed. Batches left: 126\n",
      "Batch 407/532 completed. Batches left: 125\n",
      "Batch 408/532 completed. Batches left: 124\n",
      "Batch 409/532 completed. Batches left: 123\n",
      "Batch 410/532 completed. Batches left: 122\n",
      "Batch 411/532 completed. Batches left: 121\n",
      "Batch 412/532 completed. Batches left: 120\n",
      "Batch 413/532 completed. Batches left: 119\n",
      "Batch 414/532 completed. Batches left: 118\n",
      "Batch 415/532 completed. Batches left: 117\n",
      "Batch 416/532 completed. Batches left: 116\n",
      "Batch 417/532 completed. Batches left: 115\n",
      "Batch 418/532 completed. Batches left: 114\n",
      "Batch 419/532 completed. Batches left: 113\n",
      "Batch 420/532 completed. Batches left: 112\n",
      "Batch 421/532 completed. Batches left: 111\n",
      "Batch 422/532 completed. Batches left: 110\n",
      "Batch 423/532 completed. Batches left: 109\n",
      "Batch 424/532 completed. Batches left: 108\n",
      "Batch 425/532 completed. Batches left: 107\n",
      "Batch 426/532 completed. Batches left: 106\n",
      "Batch 427/532 completed. Batches left: 105\n",
      "Batch 428/532 completed. Batches left: 104\n",
      "Batch 429/532 completed. Batches left: 103\n",
      "Batch 430/532 completed. Batches left: 102\n",
      "Batch 431/532 completed. Batches left: 101\n",
      "Batch 432/532 completed. Batches left: 100\n",
      "Batch 433/532 completed. Batches left: 99\n",
      "Batch 434/532 completed. Batches left: 98\n",
      "Batch 435/532 completed. Batches left: 97\n",
      "Batch 436/532 completed. Batches left: 96\n",
      "Batch 437/532 completed. Batches left: 95\n",
      "Batch 438/532 completed. Batches left: 94\n",
      "Batch 439/532 completed. Batches left: 93\n",
      "Batch 440/532 completed. Batches left: 92\n",
      "Batch 441/532 completed. Batches left: 91\n",
      "Batch 442/532 completed. Batches left: 90\n",
      "Batch 443/532 completed. Batches left: 89\n",
      "Batch 444/532 completed. Batches left: 88\n",
      "Batch 445/532 completed. Batches left: 87\n",
      "Batch 446/532 completed. Batches left: 86\n",
      "Batch 447/532 completed. Batches left: 85\n",
      "Batch 448/532 completed. Batches left: 84\n",
      "Batch 449/532 completed. Batches left: 83\n",
      "Batch 450/532 completed. Batches left: 82\n",
      "Batch 451/532 completed. Batches left: 81\n",
      "Batch 452/532 completed. Batches left: 80\n",
      "Batch 453/532 completed. Batches left: 79\n",
      "Batch 454/532 completed. Batches left: 78\n",
      "Batch 455/532 completed. Batches left: 77\n",
      "Batch 456/532 completed. Batches left: 76\n",
      "Batch 457/532 completed. Batches left: 75\n",
      "Batch 458/532 completed. Batches left: 74\n",
      "Batch 459/532 completed. Batches left: 73\n",
      "Batch 460/532 completed. Batches left: 72\n",
      "Batch 461/532 completed. Batches left: 71\n",
      "Batch 462/532 completed. Batches left: 70\n",
      "Batch 463/532 completed. Batches left: 69\n",
      "Batch 464/532 completed. Batches left: 68\n",
      "Batch 465/532 completed. Batches left: 67\n",
      "Batch 466/532 completed. Batches left: 66\n",
      "Batch 467/532 completed. Batches left: 65\n",
      "Batch 468/532 completed. Batches left: 64\n",
      "Batch 469/532 completed. Batches left: 63\n",
      "Batch 470/532 completed. Batches left: 62\n",
      "Batch 471/532 completed. Batches left: 61\n",
      "Batch 472/532 completed. Batches left: 60\n",
      "Batch 473/532 completed. Batches left: 59\n",
      "Batch 474/532 completed. Batches left: 58\n",
      "Batch 475/532 completed. Batches left: 57\n",
      "Batch 476/532 completed. Batches left: 56\n",
      "Batch 477/532 completed. Batches left: 55\n",
      "Batch 478/532 completed. Batches left: 54\n",
      "Batch 479/532 completed. Batches left: 53\n",
      "Batch 480/532 completed. Batches left: 52\n",
      "Batch 481/532 completed. Batches left: 51\n",
      "Batch 482/532 completed. Batches left: 50\n",
      "Batch 483/532 completed. Batches left: 49\n",
      "Batch 484/532 completed. Batches left: 48\n",
      "Batch 485/532 completed. Batches left: 47\n",
      "Batch 486/532 completed. Batches left: 46\n",
      "Batch 487/532 completed. Batches left: 45\n",
      "Batch 488/532 completed. Batches left: 44\n",
      "Batch 489/532 completed. Batches left: 43\n",
      "Batch 490/532 completed. Batches left: 42\n",
      "Batch 491/532 completed. Batches left: 41\n",
      "Batch 492/532 completed. Batches left: 40\n",
      "Batch 493/532 completed. Batches left: 39\n",
      "Batch 494/532 completed. Batches left: 38\n",
      "Batch 495/532 completed. Batches left: 37\n",
      "Batch 496/532 completed. Batches left: 36\n",
      "Batch 497/532 completed. Batches left: 35\n",
      "Batch 498/532 completed. Batches left: 34\n",
      "Batch 499/532 completed. Batches left: 33\n",
      "Batch 500/532 completed. Batches left: 32\n",
      "Batch 501/532 completed. Batches left: 31\n",
      "Batch 502/532 completed. Batches left: 30\n",
      "Batch 503/532 completed. Batches left: 29\n",
      "Batch 504/532 completed. Batches left: 28\n",
      "Batch 505/532 completed. Batches left: 27\n",
      "Batch 506/532 completed. Batches left: 26\n",
      "Batch 507/532 completed. Batches left: 25\n",
      "Batch 508/532 completed. Batches left: 24\n",
      "Batch 509/532 completed. Batches left: 23\n",
      "Batch 510/532 completed. Batches left: 22\n",
      "Batch 511/532 completed. Batches left: 21\n",
      "Batch 512/532 completed. Batches left: 20\n",
      "Batch 513/532 completed. Batches left: 19\n",
      "Batch 514/532 completed. Batches left: 18\n",
      "Batch 515/532 completed. Batches left: 17\n",
      "Batch 516/532 completed. Batches left: 16\n",
      "Batch 517/532 completed. Batches left: 15\n",
      "Batch 518/532 completed. Batches left: 14\n",
      "Batch 519/532 completed. Batches left: 13\n",
      "Batch 520/532 completed. Batches left: 12\n",
      "Batch 521/532 completed. Batches left: 11\n",
      "Batch 522/532 completed. Batches left: 10\n",
      "Batch 523/532 completed. Batches left: 9\n",
      "Batch 524/532 completed. Batches left: 8\n",
      "Batch 525/532 completed. Batches left: 7\n",
      "Batch 526/532 completed. Batches left: 6\n",
      "Batch 527/532 completed. Batches left: 5\n",
      "Batch 528/532 completed. Batches left: 4\n",
      "Batch 529/532 completed. Batches left: 3\n",
      "Batch 530/532 completed. Batches left: 2\n",
      "Batch 531/532 completed. Batches left: 1\n",
      "Batch 532/532 completed. Batches left: 0\n",
      "['NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "#2023/5/02 03:58 PM WORKING WITH Python 3.9.6 (WINDOWS non conda)\n",
    "\n",
    "\n",
    "# code wtih cuda reporting.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\" if cuda_available else \"CUDA not available\")\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\").to(\"cuda\" if cuda_available else \"cpu\")\n",
    "\n",
    "# Create a list of 17,000 sample texts (replace this with your actual data)\n",
    "texts = [\"Sample text {}\".format(i) for i in range(17000)]\n",
    "\n",
    "# Set the batch size (you may need to adjust this based on your GPU memory)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to classify sentiment in a batch of texts\n",
    "def classify_sentiment(texts_batch):\n",
    "    encoded_input = tokenizer(texts_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {key: value.to(\"cuda\" if cuda_available else \"cpu\") for key, value in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded_input).logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    labels = [\"NEGATIVE\" if p < 0.5 else \"POSITIVE\" for p in probabilities[:, 1]]\n",
    "    return labels\n",
    "\n",
    "# Process the texts in batches\n",
    "sentiments = []\n",
    "num_batches = len(texts) // batch_size + int(len(texts) % batch_size > 0)\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    texts_batch = texts[i:i + batch_size]\n",
    "    sentiments_batch = classify_sentiment(texts_batch)\n",
    "    sentiments.extend(sentiments_batch)\n",
    "    \n",
    "    # Print progress\n",
    "    batches_completed = i // batch_size + 1\n",
    "    batches_left = num_batches - batches_completed\n",
    "    print(f\"Batch {batches_completed}/{num_batches} completed. Batches left: {batches_left}\")\n",
    "\n",
    "# Print the first 10 sentiment labels\n",
    "print(sentiments[:50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is good', 'real bad', 'I like what happned', 'i hated that person', 'cats have 4 legs', 'I have shorted the stock, cuz i see a slowdown in business']\n"
     ]
    }
   ],
   "source": [
    "texts_raw = '''\n",
    "this is good\n",
    "real bad\n",
    "I like what happned\n",
    "i hated that person\n",
    "cats have 4 legs\n",
    "I have shorted the stock, cuz i see a slowdown in business\n",
    "'''\n",
    "\n",
    "texts = texts_raw.split('\\n')\n",
    "texts = [i for i in texts if i != '']\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is good', 'real bad', 'I like what happned', 'i hated that person', 'cats have 4 legs', 'I have shorted the stock, cuz i see a slowdown in business']\n",
      "tensor([[0.0086, 0.0611, 0.9303],\n",
      "        [0.6685, 0.2612, 0.0703],\n",
      "        [0.0097, 0.1706, 0.8197],\n",
      "        [0.9062, 0.0784, 0.0155],\n",
      "        [0.0499, 0.7933, 0.1569],\n",
      "        [0.6559, 0.3242, 0.0200]], device='cuda:0')\n",
      "Batch 1/1 completed. Batches left: 0\n",
      "['positive', 'negative', 'positive', 'negative', 'neutral', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# 2023/5/02 03:59 PM --- trying to use roberta updated  (model_name = f'cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "# adding some simple sentences to test roberta with CUDA\n",
    "\n",
    "# code wtih cuda reporting.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\" if cuda_available else \"CUDA not available\")\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "model_name = f'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cuda\" if cuda_available else \"cpu\")\n",
    "\n",
    "# Create a list of 17,000 sample texts (replace this with your actual data)\n",
    "# texts = [\"Sample text {}\".format(i) for i in range(17000)] #commented out to use my own text\n",
    "\n",
    "# Set the batch size (you may need to adjust this based on your GPU memory)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to classify sentiment in a batch of texts\n",
    "def classify_sentiment(texts_batch):\n",
    "    print(texts_batch)\n",
    "    encoded_input = tokenizer(texts_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {key: value.to(\"cuda\" if cuda_available else \"cpu\") for key, value in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded_input).logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    print(probabilities)\n",
    "    # labels = [\"NEGATIVE\" if p < 0.5 else \"POSITIVE\" for p in probabilities[:, 1]]\n",
    "    max_indices = torch.argmax(probabilities, dim=1)\n",
    "    # Map indices to labels\n",
    "    labels_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    labels = [labels_map[index.item()] for index in max_indices]\n",
    "    return labels\n",
    "    # return probabilities\n",
    "\n",
    "# Process the texts in batches\n",
    "sentiments = []\n",
    "num_batches = len(texts) // batch_size + int(len(texts) % batch_size > 0)\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    texts_batch = texts[i:i + batch_size]\n",
    "    sentiments_batch = classify_sentiment(texts_batch)\n",
    "    sentiments.extend(sentiments_batch)\n",
    "    \n",
    "    # Print progress\n",
    "    batches_completed = i // batch_size + 1\n",
    "    batches_left = num_batches - batches_completed\n",
    "    print(f\"Batch {batches_completed}/{num_batches} completed. Batches left: {batches_left}\")\n",
    "\n",
    "# Print the first 10 sentiment labels\n",
    "print(sentiments[:50])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading 300k tweets and getting sentiment on each one then saving that to CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 300K Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is good', 'real bad', 'I like what happned', 'i hated that person', 'cats have 4 legs', 'I have shorted the stock, cuz i see a slowdown in business']\n",
      "tensor([[0.0086, 0.0611, 0.9303],\n",
      "        [0.6685, 0.2612, 0.0703],\n",
      "        [0.0097, 0.1706, 0.8197],\n",
      "        [0.9062, 0.0784, 0.0155],\n",
      "        [0.0499, 0.7933, 0.1569],\n",
      "        [0.6559, 0.3242, 0.0200]], device='cuda:0')\n",
      "Batch 1/1 completed. Batches left: 0\n",
      "['positive', 'negative', 'positive', 'negative', 'neutral', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# WORKING WITH CUDA and adding 300k tweets to test\n",
    "\n",
    "# code wtih cuda reporting.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\" if cuda_available else \"CUDA not available\")\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "model_name = f'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name).to(\"cuda\" if cuda_available else \"cpu\")\n",
    "\n",
    "# Create a list of 17,000 sample texts (replace this with your actual data)\n",
    "# texts = [\"Sample text {}\".format(i) for i in range(17000)] #commented out to use my own text\n",
    "\n",
    "# Set the batch size (you may need to adjust this based on your GPU memory)\n",
    "batch_size = 32\n",
    "\n",
    "# Function to classify sentiment in a batch of texts\n",
    "def classify_sentiment(texts_batch):\n",
    "    print(texts_batch)\n",
    "    encoded_input = tokenizer(texts_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    encoded_input = {key: value.to(\"cuda\" if cuda_available else \"cpu\") for key, value in encoded_input.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoded_input).logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    print(probabilities)\n",
    "    # labels = [\"NEGATIVE\" if p < 0.5 else \"POSITIVE\" for p in probabilities[:, 1]]\n",
    "    max_indices = torch.argmax(probabilities, dim=1)\n",
    "    # Map indices to labels\n",
    "    labels_map = {0: 'negative', 1: 'neutral', 2: 'positive'}\n",
    "    labels = [labels_map[index.item()] for index in max_indices]\n",
    "    return labels\n",
    "    # return probabilities\n",
    "\n",
    "# Process the texts in batches\n",
    "sentiments = []\n",
    "num_batches = len(texts) // batch_size + int(len(texts) % batch_size > 0)\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    texts_batch = texts[i:i + batch_size]\n",
    "    sentiments_batch = classify_sentiment(texts_batch)\n",
    "    sentiments.extend(sentiments_batch)\n",
    "    \n",
    "    # Print progress\n",
    "    batches_completed = i // batch_size + 1\n",
    "    batches_left = num_batches - batches_completed\n",
    "    print(f\"Batch {batches_completed}/{num_batches} completed. Batches left: {batches_left}\")\n",
    "\n",
    "# Print the first 10 sentiment labels\n",
    "print(sentiments[:50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentiment back into main df\n",
    "\n",
    "\n",
    "# save main 300ktweetdf to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
