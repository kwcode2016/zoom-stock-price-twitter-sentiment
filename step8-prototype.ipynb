{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype for Twitter Sentiment Analysis for Zoom Stock\n",
    "\n",
    "This Prototype will implement the skeleton of the final project.\n",
    "\n",
    "From Step 7, this step will use SVM and RoBERTA (non-fine tuned) to analyze 5000 zoom stock tweets.\n",
    "\n",
    "\n",
    "using labeled tweets to train SVM and sentiment analysis  of zoom tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        date  \\\n",
      "0  2020-04-14 23:59:44+00:00   \n",
      "1  2020-04-14 23:54:15+00:00   \n",
      "2  2020-04-14 23:38:17+00:00   \n",
      "3  2020-04-14 23:27:05+00:00   \n",
      "4  2020-04-14 23:23:10+00:00   \n",
      "\n",
      "                                            cashtags  \n",
      "0                                             ['ZM']  \n",
      "1                                             ['ZM']  \n",
      "2                                             ['ZM']  \n",
      "3                                             ['ZM']  \n",
      "4  ['SPX', 'SPX', 'TSLA', 'NVDA', 'ZM', 'AMZN', '...  \n",
      "                           date  \\\n",
      "5679  2020-04-01 00:28:56+00:00   \n",
      "5680  2020-04-01 00:23:37+00:00   \n",
      "5681  2020-04-01 00:19:37+00:00   \n",
      "5682  2020-04-01 00:19:18+00:00   \n",
      "5683  2020-04-01 00:03:24+00:00   \n",
      "\n",
      "                                            cashtags  \n",
      "5679                                  ['GOOG', 'ZM']  \n",
      "5680  ['TSLA', 'BYND', 'ZM', 'DOCU', 'NFLX', 'TDOC']  \n",
      "5681                                  ['ZM', 'TSLA']  \n",
      "5682           ['ZM', 'MSFT', 'WORK', 'RNG', 'DOCU']  \n",
      "5683                                          ['ZM']  \n",
      "(5684, 29)\n"
     ]
    }
   ],
   "source": [
    "# load tweets. note that final project will have all tweets that have sentiment analysis done.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# zm_tweet_df = pd.read_csv('zm-tweet-2020-2-26-to-2020-2-29.csv')\n",
    "zm_tweet_df = pd.read_csv('zm-tweets-jan to april 2020.csv')\n",
    "\n",
    "\n",
    "# print(zm_tweet_df.head())\n",
    "\n",
    "print(zm_tweet_df[['date', 'cashtags']].head())\n",
    "\n",
    "# print(zm_tweet_df['date'].tail())\n",
    "print(zm_tweet_df[['date', 'cashtags']].tail())\n",
    "\n",
    "print(zm_tweet_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading labelled sentiment twitter posts\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "\n",
    "labeled_tweet_df = pd.read_csv(\"dataset-labled-tweets-original.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# cleaning tweets for sentiment analysis\n",
    "import re\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)|😉', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Tokenize the word\n",
    "    tokens = nltk.word_tokenize(word)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_word = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_word\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    processed_tweet = []\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #Clean only digits\n",
    "    tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    \n",
    "    # Replaces URLs with the word URL\n",
    "    #tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', tweet)\n",
    "    \n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    #tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    tweet = re.sub(r'@[\\S]+', '', tweet)\n",
    "    \n",
    "    # Replaces #hashtag with hashtag\n",
    "    #tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    tweet = re.sub(r'#(\\S+)', '', tweet)\n",
    "    \n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    \n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    \n",
    "    # Strip space, \" and ' from tweet\n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    #tweet = handle_emojis(tweet)\n",
    "    tweet = remove_emoji(tweet)\n",
    "   \n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "\n",
    "    #my custom chars\n",
    "    tweet = tweet.replace('₺','')\n",
    "    tweet = tweet.replace('=','')\n",
    "    tweet = tweet.replace('’','')\n",
    "    tweet = tweet.replace('|','')\n",
    "    tweet = tweet.replace('‘','')\n",
    "    tweet = tweet.replace('/','')\n",
    "    tweet = tweet.replace('…','')\n",
    "    tweet = tweet.replace('–','')\n",
    "    tweet = tweet.replace('&','')\n",
    "    tweet = tweet.replace('“','')\n",
    "    tweet = tweet.replace('”','')\n",
    "    tweet = tweet.replace('+','')\n",
    "    tweet = tweet.replace('%','')\n",
    "    tweet = tweet.replace('@','')\n",
    "    tweet = tweet.replace('#','')\n",
    "\n",
    "    words = word_tokenize(tweet) #tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "      word = preprocess_word(word)\n",
    "      #if is_valid_word(word):\n",
    "      #    processed_tweet.append(word)\n",
    "      processed_tweet.append(word)\n",
    "\n",
    "    return ' '.join(processed_tweet)\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    handle_emojis(tweet)\n",
    "    remove_emoji(tweet)\n",
    "    preprocess_tweet(tweet)\n",
    "    return str(tweet) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweet_df[\"clean_tweet\"] = labeled_tweet_df['text'].apply(clean_tweet) # applies clean_tweet function on each record of 'text' and creates a new column 'clean_tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \\\n",
       "0  I`d have responded, if I were going   neutral   \n",
       "1                             Sooo SAD  negative   \n",
       "2                          bullying me  negative   \n",
       "3                       leave me alone  negative   \n",
       "4                        Sons of ****,  negative   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0                I`d have responded, if I were going  \n",
       "1      Sooo SAD I will miss you here in San Diego!!!  \n",
       "2                          my boss is bullying me...  \n",
       "3                     what interview! leave me alone  \n",
       "4   Sons of ****, why couldn`t they put them on t...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27481, 5)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            textID                                               text  \\\n",
       "9879   81a05efb02                        I`am back to Tweetie 1.1.1.   \n",
       "18790  26149e0547      Cross Country was today...Hmmphh my feet hurt   \n",
       "9134   035e3b10ef        Taking my brother to the airport  Then gym.   \n",
       "6649   c4aa8ca103                                Raining in Calicut.   \n",
       "2669   1e8baf3c3e     had a **** contact with both parents >.>  grrr   \n",
       "...           ...                                                ...   \n",
       "10159  c774e791ab  Ugh.  Las #vegas airport is at a 'ground stop'...   \n",
       "25194  04682e718c                            really wants a puppy...   \n",
       "7945   b804b59377   that is sickening... Just shoot at will? Smh....   \n",
       "12990  c280ab00dd   glad its nice there. Gray skies and rainy in ...   \n",
       "22174  7d85365781  _sis He got those cars before he lost his job....   \n",
       "\n",
       "                                           selected_text sentiment  \\\n",
       "9879                         I`am back to Tweetie 1.1.1.   neutral   \n",
       "18790                                               hurt  negative   \n",
       "9134         Taking my brother to the airport  Then gym.   neutral   \n",
       "6649                                 Raining in Calicut.   neutral   \n",
       "2669                                                ****  negative   \n",
       "...                                                  ...       ...   \n",
       "10159  I`m stuck in the plane on the tarmac (again) a...  negative   \n",
       "25194                            really wants a puppy...   neutral   \n",
       "7945                                  that is sickening.  negative   \n",
       "12990                               glad its nice there.  positive   \n",
       "22174  _sis He got those cars before he lost his job....   neutral   \n",
       "\n",
       "                                             clean_tweet  \n",
       "9879                         I`am back to Tweetie 1.1.1.  \n",
       "18790      Cross Country was today...Hmmphh my feet hurt  \n",
       "9134         Taking my brother to the airport  Then gym.  \n",
       "6649                                 Raining in Calicut.  \n",
       "2669      had a **** contact with both parents >.>  grrr  \n",
       "...                                                  ...  \n",
       "10159  Ugh.  Las #vegas airport is at a 'ground stop'...  \n",
       "25194                            really wants a puppy...  \n",
       "7945    that is sickening... Just shoot at will? Smh....  \n",
       "12990   glad its nice there. Gray skies and rainy in ...  \n",
       "22174  _sis He got those cars before he lost his job....  \n",
       "\n",
       "[5497 rows x 5 columns]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(labeled_tweet_df, test_size=0.2)\n",
    "test.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "BOW = vectorizer.fit_transform(labeled_tweet_df['clean_tweet'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(labeled_tweet_df[\"sentiment\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "6871\n"
     ]
    }
   ],
   "source": [
    "print(type(predictions))\n",
    "\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model is 67.37010624363265%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "print(\"Accuracy of model is {}%\".format(accuracy_score(y_test,predictions) * 100))\n",
    "\n",
    "\n",
    "# note that accuracy changes each time: so far ranging from 67.37% to 69%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Zoom Tweets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning zoom tweets using the following method\n",
    "# df[\"clean_tweet\"] = df['text'].apply(clean_tweet) # applies clean_tweet function on each record of 'text' and creates a new column 'clean_tweet'\n",
    "\n",
    "zm_tweet_df['clean_tweet'] = zm_tweet_df['content'].apply(clean_tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>cashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5679</th>\n",
       "      <td>My kids are getting an incredible crash course...</td>\n",
       "      <td>['GOOG', 'ZM']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5680</th>\n",
       "      <td>Fellow #StockNerds...this one thing determines...</td>\n",
       "      <td>['TSLA', 'BYND', 'ZM', 'DOCU', 'NFLX', 'TDOC']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5681</th>\n",
       "      <td>$ZM is like $TSLA without the \\n* IP\\n* Unique...</td>\n",
       "      <td>['ZM', 'TSLA']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5682</th>\n",
       "      <td>$ZM Zoom's active users up 151% this month  ht...</td>\n",
       "      <td>['ZM', 'MSFT', 'WORK', 'RNG', 'DOCU']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5683</th>\n",
       "      <td>Thought about selling my $ZM shares until I le...</td>\n",
       "      <td>['ZM']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            clean_tweet  \\\n",
       "5679  My kids are getting an incredible crash course...   \n",
       "5680  Fellow #StockNerds...this one thing determines...   \n",
       "5681  $ZM is like $TSLA without the \\n* IP\\n* Unique...   \n",
       "5682  $ZM Zoom's active users up 151% this month  ht...   \n",
       "5683  Thought about selling my $ZM shares until I le...   \n",
       "\n",
       "                                            cashtags  \n",
       "5679                                  ['GOOG', 'ZM']  \n",
       "5680  ['TSLA', 'BYND', 'ZM', 'DOCU', 'NFLX', 'TDOC']  \n",
       "5681                                  ['ZM', 'TSLA']  \n",
       "5682           ['ZM', 'MSFT', 'WORK', 'RNG', 'DOCU']  \n",
       "5683                                          ['ZM']  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zm_tweet_df[['clean_tweet', 'cashtags']].tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Trained SVM Model for sentiment analysis for 5k Zoom Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6871x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 74798 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run svm on 5k dataset \n",
    "    # create a new column \"svm_zm_sentiment\" on zm_tweet_df['clean_tweet']\n",
    "        #setup SVM for sentiment analysis - setting up Bag of Words (BOW) - note that this was all done earlier\n",
    "new_BOW = vectorizer.transform(zm_tweet_df['clean_tweet'])\n",
    "\n",
    "        #create the ndarray predictions_svm_zm to be added as a new column back into zm_tweet_df\n",
    "predictions_svm_zm = model.predict(new_BOW)\n",
    "        #creating the new column 'svm_sentiment'\n",
    "zm_tweet_df['sentiment_svm'] = predictions_svm_zm\n",
    "\n",
    "\n",
    "# about 60 sec for SVM for 5k tweets note how much time passed. to compare to RoBERTa sentiment analysis\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     5094\n",
       "positive     423\n",
       "negative     167\n",
       "Name: sentiment_svm, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at svm_sentiment\n",
    "\n",
    "zm_tweet_df[['date', 'clean_tweet', 'sentiment_svm']].tail()\n",
    "\n",
    "zm_tweet_df['sentiment_svm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save to entire df to csv 'zm_5k_tweets_step1_with_svm.csv'\n",
    "zm_tweet_df.to_csv('zm_5k_tweets_step1_with_svm.csv')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa - no finetuning twitter analysis on 5k twitter dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to load RoBERTa from step 7 (this step, no time to use CUDA cores will use old slow for loop method)\n",
    "# load zm_5k_tweets_step1_with_svm.csv into zm_tweet_df (run this if not running from start of notebook)\n",
    "\n",
    "\n",
    "# uncomment if running from start\n",
    "zm_tweet_df = pd.read_csv('zm_5k_tweets_step1_with_svm.csv') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for RoBERTa \n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MODEL string: cardiffnlp/twitter-roberta-base-sentiment\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "print(f\"\\n\\nMODEL string: {MODEL}\\n\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on text: $zm ended the day at the price of $90.95\n",
      "ranking: [0 2 1]\n",
      "ranking: [1 2 0]\n",
      "1) neutral 0.896\n",
      "2) positive 0.0624\n",
      "3) negative 0.0417\n"
     ]
    }
   ],
   "source": [
    "# Testing individual tweets\n",
    "\n",
    "text = \"$zm ended the day at the price of $90.95\"\n",
    "text = clean_tweet(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "print(f\"Results on text: {text}\")\n",
    "\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "\n",
    "print(\"ranking: \" + str(ranking))\n",
    "ranking = ranking[::-1]\n",
    "print(\"ranking: \" + str(ranking))\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_R = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model_r = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5654     neutral\n",
       "5655    negative\n",
       "5656     neutral\n",
       "5657     neutral\n",
       "5658     neutral\n",
       "5659     neutral\n",
       "5660     neutral\n",
       "5661     neutral\n",
       "5662     neutral\n",
       "5663     neutral\n",
       "5664     neutral\n",
       "5665     neutral\n",
       "5666     neutral\n",
       "5667     neutral\n",
       "5668     neutral\n",
       "5669     neutral\n",
       "5670     neutral\n",
       "5671     neutral\n",
       "5672     neutral\n",
       "5673     neutral\n",
       "5674    positive\n",
       "5675     neutral\n",
       "5676     neutral\n",
       "5677     neutral\n",
       "5678     neutral\n",
       "5679    negative\n",
       "5680     neutral\n",
       "5681     neutral\n",
       "5682     neutral\n",
       "5683     neutral\n",
       "Name: sentiment_svm, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zm_tweet_df['clean_tweet'].tail()\n",
    "\n",
    "\n",
    "zm_tweet_df['sentiment_svm'].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RoBERTa predicted sentiments\n",
    "predictions = []\n",
    "\n",
    "for text in zm_tweet_df['clean_tweet']:\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model_r(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    predicted_label = config.id2label[ranking[0]]\n",
    "    predictions.append(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print display after each 50 processed.. print row_num + seconds_passed\n",
    "# when complete. note how much time passed. compare to SVM time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep to add new col 'r_sentiment'\n",
    "df_predictions_r_series = pd.Series(predictions,  name='sentiment_r') # creating temp df series \n",
    "\n",
    "# create a new column in 5k zoom tweet 'r_sentiment'\n",
    "zm_tweet_df = zm_tweet_df.assign(sentiment_r = df_predictions_r_series) # creating the new col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if all the sentiment cols are there\n",
    "zm_tweet_df[['clean_tweet', 'sentiment_svm', 'sentiment_r']].tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save entire df with 'zm_5k_tweets_with_svm_and_r.csv'\n",
    "zm_tweet_df.to_csv('zm_5k_tweets_step2_with_svn_and_roberta.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare differences in sentiment analysis between SVM and RoBERTa\n",
    "\n",
    "This will help me formulate the best way to finetune RoBERTa to be more accurate.\n",
    "\n",
    "If there is time, we should finetune using the labeled tweets too. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df from 'zm_5k_tweets_step2_with_svn_and_roberta.csv'\n",
    "zm_tweet_df = pd.read_csv('zm_5k_tweets_step2_with_svn_and_roberta.csv')\n",
    "\n",
    "# quick check - svm -- how many positive, neutral and negative using the 1 line code\n",
    "zm_tweet_df['sentiment_svm'].value_counts()\n",
    "# quick check - roberta -- how many positive, neutral and negative\n",
    "zm_tweet_df['sentiment_r'].value_counts()\n",
    "# quick check - how many svm and roberta MATCHES\n",
    "# quick check - how  many svm and roberta DOESN'T Match\n",
    "\n",
    "# create a new column final_sentiment\n",
    "    # if svm_sentiment == r_sentiment: update final_sentiment = r_sentiment \n",
    "    # at this point, there will be lots of blanks where svm_sentiment != r_sentiment\n",
    "\n",
    "# detailed list of MISMATCH- \n",
    "    # if svm_sentiment != r_sentiment: print (clean_tweet, svm_sentiment, r_sentiment)\n",
    "    # manually look at the tweet and update final_sentiment (the other matching sentiments should already filled)\n",
    "\n",
    "# Save df to 'zm_5k_tweets_with_final_sentiment.csv'\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINETUNE RoBERTa with final_sentiment. \n",
    "I assume that the svm and robeta match are already trained in the model.\n",
    "\n",
    "Finetune using the mismatch rows with the new final_sentiment row.\n",
    "\n",
    "Save the finetuned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 'zm_5k_tweets_with_final_sentiment.csv'\n",
    "# I assume that the svm and robeta match are already trained in the model.\n",
    "\n",
    "# Finetune using the mismatch rows with the new final_sentiment row.\n",
    "\n",
    "# Save the finetuned data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
