{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with various models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting using the Support Vector Machine algorithm and Bag of Words representation\n",
    "\n",
    "I will be using a clean and labeled tweet dataset from Kaggle. It has just over 27k tweets labeled with sentiment.\n",
    "\n",
    "Before implementing my main roBERTa model for twitter sentiment analysis, it is always good to test other more traditional machine learning methods.\n",
    "\n",
    "One really good method is Support Vector Machine (SVM). I really like how it extends into higher dimensions to find groupings. And I will be using the simple Bag of Words representation on the 27k tweets.\n",
    "\n",
    "\n",
    "After that, I will be implement roBERTa with no fine tuning to comapre the difference in accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment  \n",
      "0  I`d have responded, if I were going   neutral  \n",
      "1                             Sooo SAD  negative  \n",
      "2                          bullying me  negative  \n",
      "3                       leave me alone  negative  \n",
      "4                        Sons of ****,  negative  \n",
      "(27481, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "\n",
    "df = pd.read_csv(\"dataset-labled-tweets-original.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# cleaning tweets for sentiment analysis\n",
    "import re\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)|😉', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n",
    "                           u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           u\"\\U0001f926-\\U0001f937\"\n",
    "                           u\"\\U00010000-\\U0010ffff\"\n",
    "                           u\"\\u2640-\\u2642\"\n",
    "                           u\"\\u2600-\\u2B55\"\n",
    "                           u\"\\u200d\"\n",
    "                           u\"\\u23cf\"\n",
    "                           u\"\\u23e9\"\n",
    "                           u\"\\u231a\"\n",
    "                           u\"\\ufe0f\"  # dingbats\n",
    "                           u\"\\u3030\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', string)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Tokenize the word\n",
    "    tokens = nltk.word_tokenize(word)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem the tokens\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    preprocessed_word = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_word\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    processed_tweet = []\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #Clean only digits\n",
    "    tweet = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", tweet)\n",
    "    \n",
    "    # Replaces URLs with the word URL\n",
    "    #tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', '', tweet)\n",
    "    \n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    #tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    tweet = re.sub(r'@[\\S]+', '', tweet)\n",
    "    \n",
    "    # Replaces #hashtag with hashtag\n",
    "    #tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    tweet = re.sub(r'#(\\S+)', '', tweet)\n",
    "    \n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    \n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    \n",
    "    # Strip space, \" and ' from tweet\n",
    "    tweet = tweet.strip(' \"\\'')\n",
    "\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    #tweet = handle_emojis(tweet)\n",
    "    tweet = remove_emoji(tweet)\n",
    "   \n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "\n",
    "    #my custom chars\n",
    "    tweet = tweet.replace('₺','')\n",
    "    tweet = tweet.replace('=','')\n",
    "    tweet = tweet.replace('’','')\n",
    "    tweet = tweet.replace('|','')\n",
    "    tweet = tweet.replace('‘','')\n",
    "    tweet = tweet.replace('/','')\n",
    "    tweet = tweet.replace('…','')\n",
    "    tweet = tweet.replace('–','')\n",
    "    tweet = tweet.replace('&','')\n",
    "    tweet = tweet.replace('“','')\n",
    "    tweet = tweet.replace('”','')\n",
    "    tweet = tweet.replace('+','')\n",
    "    tweet = tweet.replace('%','')\n",
    "    tweet = tweet.replace('@','')\n",
    "    tweet = tweet.replace('#','')\n",
    "\n",
    "    words = word_tokenize(tweet) #tweet.split()\n",
    "\n",
    "    for word in words:\n",
    "      word = preprocess_word(word)\n",
    "      #if is_valid_word(word):\n",
    "      #    processed_tweet.append(word)\n",
    "      processed_tweet.append(word)\n",
    "\n",
    "    return ' '.join(processed_tweet)\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    handle_emojis(tweet)\n",
    "    remove_emoji(tweet)\n",
    "    preprocess_tweet(tweet)\n",
    "    return str(tweet) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_tweet\"] = df['text'].apply(clean_tweet) # applies clean_tweet function on each record of 'text' and creates a new column 'clean_tweet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \\\n",
       "0  I`d have responded, if I were going   neutral   \n",
       "1                             Sooo SAD  negative   \n",
       "2                          bullying me  negative   \n",
       "3                       leave me alone  negative   \n",
       "4                        Sons of ****,  negative   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0                I`d have responded, if I were going  \n",
       "1      Sooo SAD I will miss you here in San Diego!!!  \n",
       "2                          my boss is bullying me...  \n",
       "3                     what interview! leave me alone  \n",
       "4   Sons of ****, why couldn`t they put them on t...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of            textID                                               text  \\\n",
       "4257   2dc2b2ecaf                        i love you <3  host me lmao   \n",
       "11686  a3f50ce6ea  My guitar ain`t here yet?, feel like i lost a ...   \n",
       "6320   b58106c8bd                            This world makes me sad   \n",
       "22062  b8eb46f4c4                                    ill buy you one   \n",
       "21238  e7a0cfb127  soo tired.. still kinda angry that i missed th...   \n",
       "...           ...                                                ...   \n",
       "22979  29cb8acb08   Aww ya not showing off all us mums should be ...   \n",
       "19270  78e0057a31  My dog is suffering from abandonment issues. S...   \n",
       "11611  cfe1e3bf7e            ahhh twitter, I havent seen you all day   \n",
       "17542  7f3951027f   cool. my **** itch. got sunburned at the volcano   \n",
       "23554  a3e4471cef           Working at hop city. Gotta miss baseball   \n",
       "\n",
       "                                           selected_text sentiment  \\\n",
       "4257                                                love  positive   \n",
       "11686                                               lost  negative   \n",
       "6320                                                 sad  negative   \n",
       "22062                                    ill buy you one   neutral   \n",
       "21238  soo tired.. still kinda angry that i missed th...  negative   \n",
       "...                                                  ...       ...   \n",
       "22979  Aww ya not showing off all us mums should be p...  positive   \n",
       "19270                                          suffering  negative   \n",
       "11611            ahhh twitter, I havent seen you all day   neutral   \n",
       "17542                       got sunburned at the volcano  negative   \n",
       "23554                                Gotta miss baseball  negative   \n",
       "\n",
       "                                             clean_tweet  \n",
       "4257                         i love you <3  host me lmao  \n",
       "11686  My guitar ain`t here yet?, feel like i lost a ...  \n",
       "6320                             This world makes me sad  \n",
       "22062                                    ill buy you one  \n",
       "21238  soo tired.. still kinda angry that i missed th...  \n",
       "...                                                  ...  \n",
       "22979   Aww ya not showing off all us mums should be ...  \n",
       "19270  My dog is suffering from abandonment issues. S...  \n",
       "11611            ahhh twitter, I havent seen you all day  \n",
       "17542   cool. my **** itch. got sunburned at the volcano  \n",
       "23554           Working at hop city. Gotta miss baseball  \n",
       "\n",
       "[5497 rows x 5 columns]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "test.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "BOW = vectorizer.fit_transform(df['clean_tweet'])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(BOW,np.asarray(df[\"sentiment\"]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model is 69.04380730606898%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "print(\"Accuracy of model is {}%\".format(accuracy_score(y_test,predictions) * 100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MODEL string: cardiffnlp/twitter-roberta-base-sentiment\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "print(f\"\\n\\nMODEL string: {MODEL}\\n\\n\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on text: $zm ended the day at the price of $90.95\n",
      "ranking: [0 2 1]\n",
      "ranking: [1 2 0]\n",
      "1) neutral 0.896\n",
      "2) positive 0.0624\n",
      "3) negative 0.0417\n"
     ]
    }
   ],
   "source": [
    "# Testing individual tweets\n",
    "\n",
    "text = \"$zm ended the day at the price of $90.95\"\n",
    "text = clean_tweet(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "\n",
    "print(f\"Results on text: {text}\")\n",
    "\n",
    "\n",
    "ranking = np.argsort(scores)\n",
    "\n",
    "print(\"ranking: \" + str(ranking))\n",
    "ranking = ranking[::-1]\n",
    "print(\"ranking: \" + str(ranking))\n",
    "for i in range(scores.shape[0]):\n",
    "    l = labels[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_R = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model_r = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  I`d have responded, if I were going\n",
       "1        Sooo SAD I will miss you here in San Diego!!!\n",
       "2                            my boss is bullying me...\n",
       "3                       what interview! leave me alone\n",
       "4     Sons of ****, why couldn`t they put them on t...\n",
       "Name: clean_tweet, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_tweet'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m df[\u001b[39m'\u001b[39m\u001b[39mclean_tweet\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m      4\u001b[0m     encoded_input \u001b[39m=\u001b[39m tokenizer(text, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     output \u001b[39m=\u001b[39m model_r(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mencoded_input)\n\u001b[0;32m      6\u001b[0m     scores \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m      7\u001b[0m     scores \u001b[39m=\u001b[39m softmax(scores)\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1208\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1200\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1203\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1206\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1208\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m   1209\u001b[0m     input_ids,\n\u001b[0;32m   1210\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1211\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1212\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1213\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1214\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1215\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1216\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1217\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1218\u001b[0m )\n\u001b[0;32m   1219\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1220\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:846\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    837\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    839\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    840\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    841\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    844\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    845\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    847\u001b[0m     embedding_output,\n\u001b[0;32m    848\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    849\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    850\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    851\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    852\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    853\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    854\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    855\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    856\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    857\u001b[0m )\n\u001b[0;32m    858\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    859\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    511\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    513\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    521\u001b[0m         hidden_states,\n\u001b[0;32m    522\u001b[0m         attention_mask,\n\u001b[0;32m    523\u001b[0m         layer_head_mask,\n\u001b[0;32m    524\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    525\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    526\u001b[0m         past_key_value,\n\u001b[0;32m    527\u001b[0m         output_attentions,\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    530\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    531\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    444\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    445\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 447\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    448\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    449\u001b[0m )\n\u001b[0;32m    450\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    452\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 246\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:460\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    459\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 460\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:371\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 371\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    372\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    373\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jk\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = []\n",
    "\n",
    "for text in df['clean_tweet']:\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model_r(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    predicted_label = config.id2label[ranking[0]]\n",
    "    predictions.append(predicted_label)\n",
    "\n",
    "    # this was interrupted as the task was taking too long, but we did get 17k sentiments from RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.DataFrame(predictions, columns=['sentiment_r'])\n",
    "\n",
    "df_predictions.to_csv('17k_sentiment_r.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding 17k sentiment_r to df and comparing them.\n",
    "\n",
    "df_predictions_r_series = pd.Series(predictions,  name='sentiment_r')\n",
    "\n",
    "df = df.assign(prediction_r = df_predictions_r_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>prediction_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hes just not that into you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>oh Marly, I`m so sorry!!  I hope you find her...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Playing Ghost Online is really interesting. Th...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>is cleaning the house for her family who is co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gotta restart my computer .. I thought Win7 wa...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>the free fillin` app on my ipod is fun, im add...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>I`m sorry.</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>On the way to Malaysia...no internet access to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>juss came backk from Berkeleyy ; omg its madd ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Went to sleep and there is a power cut in Noid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_tweet sentiment prediction_r\n",
       "0                 I`d have responded, if I were going   neutral      neutral\n",
       "1       Sooo SAD I will miss you here in San Diego!!!  negative     negative\n",
       "2                           my boss is bullying me...  negative     negative\n",
       "3                      what interview! leave me alone  negative     negative\n",
       "4    Sons of ****, why couldn`t they put them on t...  negative     negative\n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral      neutral\n",
       "6   2am feedings for the baby are fun when he is a...  positive     positive\n",
       "7                                          Soooo high   neutral     positive\n",
       "8                                         Both of you   neutral      neutral\n",
       "9    Journey!? Wow... u just became cooler.  hehe....  positive     positive\n",
       "10   as much as i love to be hopeful, i reckon the...   neutral     negative\n",
       "11  I really really like the song Love Story by Ta...  positive     positive\n",
       "12       My Sharpie is running DANGERously low on ink  negative     negative\n",
       "13  i want to go to music tonight but i lost my vo...  negative     negative\n",
       "14                         test test from the LG enV2   neutral      neutral\n",
       "15                              Uh oh, I am sunburned  negative     negative\n",
       "16   S`ok, trying to plot alternatives as we speak...  negative     negative\n",
       "17  i`ve been sick for the past few days  and thus...  negative     negative\n",
       "18         is back home now      gonna miss every one  negative      neutral\n",
       "19                         Hes just not that into you   neutral     negative\n",
       "20   oh Marly, I`m so sorry!!  I hope you find her...   neutral     negative\n",
       "21  Playing Ghost Online is really interesting. Th...  positive     positive\n",
       "22  is cleaning the house for her family who is co...   neutral      neutral\n",
       "23  gotta restart my computer .. I thought Win7 wa...   neutral     negative\n",
       "24  SEe waT I Mean bOuT FoLL0w fRiiDaYs... It`S cA...   neutral     negative\n",
       "25  the free fillin` app on my ipod is fun, im add...  positive     positive\n",
       "26                                         I`m sorry.  negative     negative\n",
       "27  On the way to Malaysia...no internet access to...  negative      neutral\n",
       "28  juss came backk from Berkeleyy ; omg its madd ...  positive     positive\n",
       "29  Went to sleep and there is a power cut in Noid...  negative     negative"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['clean_tweet','sentiment', 'prediction_r']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>prediction_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17886</th>\n",
       "      <td>6ac6671968</td>\n",
       "      <td>Heh heh heh, come on! It is a THQ release! Ah...</td>\n",
       "      <td>Heh heh heh, come on! It is a THQ release! Ah ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Heh heh heh, come on! It is a THQ release! Ah...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17887</th>\n",
       "      <td>578d6c1bfe</td>\n",
       "      <td>My friend Cliff has the tix, so I have to wai...</td>\n",
       "      <td>My friend Cliff has the tix, so I have to wait...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>My friend Cliff has the tix, so I have to wai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17888</th>\n",
       "      <td>290a0ffa68</td>\n",
       "      <td>haha that`s because you also look amazing in ...</td>\n",
       "      <td>haha that`s because you also look amazing in i...</td>\n",
       "      <td>positive</td>\n",
       "      <td>haha that`s because you also look amazing in ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17889</th>\n",
       "      <td>073f1f3c0c</td>\n",
       "      <td>_Stained http://twitpic.com/4jhe5 - I LOVE it!...</td>\n",
       "      <td>LOVE</td>\n",
       "      <td>positive</td>\n",
       "      <td>_Stained http://twitpic.com/4jhe5 - I LOVE it!...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17890</th>\n",
       "      <td>c56f034be1</td>\n",
       "      <td>ive finished them now</td>\n",
       "      <td>ive finished them now</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ive finished them now</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17891</th>\n",
       "      <td>05ed6c49d8</td>\n",
       "      <td>sorry to hear about your dog</td>\n",
       "      <td>sorry</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry to hear about your dog</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17892</th>\n",
       "      <td>ff61f3369a</td>\n",
       "      <td>After a week staying with my Grandmother, I`m ...</td>\n",
       "      <td>After a week staying with my Grandmother, I`m ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>After a week staying with my Grandmother, I`m ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17893</th>\n",
       "      <td>e1cf6a99a1</td>\n",
       "      <td>since I`m reading the Twilight series and wat...</td>\n",
       "      <td>.perfect</td>\n",
       "      <td>positive</td>\n",
       "      <td>since I`m reading the Twilight series and wat...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17894</th>\n",
       "      <td>eb0e2ed274</td>\n",
       "      <td>Bummed out I am missing a rock climbing trip n...</td>\n",
       "      <td>missing</td>\n",
       "      <td>negative</td>\n",
       "      <td>Bummed out I am missing a rock climbing trip n...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17895</th>\n",
       "      <td>7ccc3bafa7</td>\n",
       "      <td>lamentablemente paso  #jrztwitterlunch</td>\n",
       "      <td>lamentablemente paso  #jrztwitterlunch</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lamentablemente paso  #jrztwitterlunch</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17896</th>\n",
       "      <td>804efc7f73</td>\n",
       "      <td>i`m having a hard time using hulu  have u hea...</td>\n",
       "      <td>hard</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m having a hard time using hulu  have u hea...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17897</th>\n",
       "      <td>e9cc6487c5</td>\n",
       "      <td>is about to go duster shoppping in greenhills ...</td>\n",
       "      <td>is about to go duster shoppping in greenhills ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is about to go duster shoppping in greenhills ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17898</th>\n",
       "      <td>b16b566d38</td>\n",
       "      <td>says SPACE AND TIME IS ALL WE NEED  http://plu...</td>\n",
       "      <td>says SPACE AND TIME IS ALL WE NEED  http://plu...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>says SPACE AND TIME IS ALL WE NEED  http://plu...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17899</th>\n",
       "      <td>55cb950187</td>\n",
       "      <td>BREAKING NEWS: GM shares are currently trading...</td>\n",
       "      <td>trading below $1.00 at only $.88 per share!!</td>\n",
       "      <td>negative</td>\n",
       "      <td>BREAKING NEWS: GM shares are currently trading...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17900</th>\n",
       "      <td>06a80bb433</td>\n",
       "      <td>Sorry I`m not using tweetdeck it`s not letting...</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>negative</td>\n",
       "      <td>Sorry I`m not using tweetdeck it`s not letting...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17901</th>\n",
       "      <td>476f9b7220</td>\n",
       "      <td>I feel like drinking wine, but I don`t have any.</td>\n",
       "      <td>I feel like drinking wine, but I don`t have any.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I feel like drinking wine, but I don`t have any.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17902</th>\n",
       "      <td>f6c8242722</td>\n",
       "      <td>you are so right!</td>\n",
       "      <td>you are so right!</td>\n",
       "      <td>positive</td>\n",
       "      <td>you are so right!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17903</th>\n",
       "      <td>c4f7024010</td>\n",
       "      <td>In denial about the moving van parked a few bl...</td>\n",
       "      <td>In denial</td>\n",
       "      <td>negative</td>\n",
       "      <td>In denial about the moving van parked a few bl...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17904</th>\n",
       "      <td>168216a4a6</td>\n",
       "      <td>not yet. im still loading it.  have you?</td>\n",
       "      <td>not yet. im still loading it.  have you?</td>\n",
       "      <td>neutral</td>\n",
       "      <td>not yet. im still loading it.  have you?</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17905</th>\n",
       "      <td>3d7b8ed35a</td>\n",
       "      <td>im so hungry and i have nothing to eat</td>\n",
       "      <td>hungry</td>\n",
       "      <td>negative</td>\n",
       "      <td>im so hungry and i have nothing to eat</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17906</th>\n",
       "      <td>3d4f5c3352</td>\n",
       "      <td>I hope for a speedy recovery for you!</td>\n",
       "      <td>I hope for a speedy recovery for you!</td>\n",
       "      <td>positive</td>\n",
       "      <td>I hope for a speedy recovery for you!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17907</th>\n",
       "      <td>adb9a99727</td>\n",
       "      <td>is starting her pre europe diet today  goodbye...</td>\n",
       "      <td>is starting her pre europe diet today  goodbye...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is starting her pre europe diet today  goodbye...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17908</th>\n",
       "      <td>007f31acef</td>\n",
       "      <td>kid`s class is going on field trip today ~ and...</td>\n",
       "      <td>kid`s class is going on field trip today ~ and...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>kid`s class is going on field trip today ~ and...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17909</th>\n",
       "      <td>db60576b3b</td>\n",
       "      <td>Well, all I can do, is pray for you.....</td>\n",
       "      <td>, is pray for you....</td>\n",
       "      <td>positive</td>\n",
       "      <td>Well, all I can do, is pray for you.....</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17910</th>\n",
       "      <td>036fd7221e</td>\n",
       "      <td>thankfully it`s just a really bad sinus infec...</td>\n",
       "      <td>thankfully it`s just a really bad sinus infect...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>thankfully it`s just a really bad sinus infec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17911</th>\n",
       "      <td>3e5f81129f</td>\n",
       "      <td>is eating lunch at Sushi Tei, Sency yum! met G...</td>\n",
       "      <td>is eating lunch at Sushi Tei, Sency yum! met G...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is eating lunch at Sushi Tei, Sency yum! met G...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17912</th>\n",
       "      <td>751983d6dd</td>\n",
       "      <td>****. She got it on the first try.</td>\n",
       "      <td>****. She got it on the first try.</td>\n",
       "      <td>neutral</td>\n",
       "      <td>****. She got it on the first try.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17913</th>\n",
       "      <td>a95d808e79</td>\n",
       "      <td>Just loaned out a set of aerobars to team mate...</td>\n",
       "      <td>Bummed I won`t be there.</td>\n",
       "      <td>negative</td>\n",
       "      <td>Just loaned out a set of aerobars to team mate...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17914</th>\n",
       "      <td>501ae0d4e7</td>\n",
       "      <td>like cheesecake brownies!!! i miss my cheesec...</td>\n",
       "      <td>miss</td>\n",
       "      <td>negative</td>\n",
       "      <td>like cheesecake brownies!!! i miss my cheesec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17915</th>\n",
       "      <td>4ba5995598</td>\n",
       "      <td>i`m so hungry</td>\n",
       "      <td>i`m so hungry</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m so hungry</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "17886  6ac6671968   Heh heh heh, come on! It is a THQ release! Ah...   \n",
       "17887  578d6c1bfe   My friend Cliff has the tix, so I have to wai...   \n",
       "17888  290a0ffa68   haha that`s because you also look amazing in ...   \n",
       "17889  073f1f3c0c  _Stained http://twitpic.com/4jhe5 - I LOVE it!...   \n",
       "17890  c56f034be1                              ive finished them now   \n",
       "17891  05ed6c49d8                       sorry to hear about your dog   \n",
       "17892  ff61f3369a  After a week staying with my Grandmother, I`m ...   \n",
       "17893  e1cf6a99a1   since I`m reading the Twilight series and wat...   \n",
       "17894  eb0e2ed274  Bummed out I am missing a rock climbing trip n...   \n",
       "17895  7ccc3bafa7             lamentablemente paso  #jrztwitterlunch   \n",
       "17896  804efc7f73   i`m having a hard time using hulu  have u hea...   \n",
       "17897  e9cc6487c5  is about to go duster shoppping in greenhills ...   \n",
       "17898  b16b566d38  says SPACE AND TIME IS ALL WE NEED  http://plu...   \n",
       "17899  55cb950187  BREAKING NEWS: GM shares are currently trading...   \n",
       "17900  06a80bb433  Sorry I`m not using tweetdeck it`s not letting...   \n",
       "17901  476f9b7220   I feel like drinking wine, but I don`t have any.   \n",
       "17902  f6c8242722                                 you are so right!    \n",
       "17903  c4f7024010  In denial about the moving van parked a few bl...   \n",
       "17904  168216a4a6           not yet. im still loading it.  have you?   \n",
       "17905  3d7b8ed35a             im so hungry and i have nothing to eat   \n",
       "17906  3d4f5c3352              I hope for a speedy recovery for you!   \n",
       "17907  adb9a99727  is starting her pre europe diet today  goodbye...   \n",
       "17908  007f31acef  kid`s class is going on field trip today ~ and...   \n",
       "17909  db60576b3b           Well, all I can do, is pray for you.....   \n",
       "17910  036fd7221e   thankfully it`s just a really bad sinus infec...   \n",
       "17911  3e5f81129f  is eating lunch at Sushi Tei, Sency yum! met G...   \n",
       "17912  751983d6dd                 ****. She got it on the first try.   \n",
       "17913  a95d808e79  Just loaned out a set of aerobars to team mate...   \n",
       "17914  501ae0d4e7   like cheesecake brownies!!! i miss my cheesec...   \n",
       "17915  4ba5995598                                      i`m so hungry   \n",
       "\n",
       "                                           selected_text sentiment  \\\n",
       "17886  Heh heh heh, come on! It is a THQ release! Ah ...   neutral   \n",
       "17887  My friend Cliff has the tix, so I have to wait...   neutral   \n",
       "17888  haha that`s because you also look amazing in i...  positive   \n",
       "17889                                               LOVE  positive   \n",
       "17890                              ive finished them now   neutral   \n",
       "17891                                              sorry  negative   \n",
       "17892  After a week staying with my Grandmother, I`m ...   neutral   \n",
       "17893                                           .perfect  positive   \n",
       "17894                                            missing  negative   \n",
       "17895             lamentablemente paso  #jrztwitterlunch   neutral   \n",
       "17896                                               hard  negative   \n",
       "17897  is about to go duster shoppping in greenhills ...   neutral   \n",
       "17898  says SPACE AND TIME IS ALL WE NEED  http://plu...   neutral   \n",
       "17899       trading below $1.00 at only $.88 per share!!  negative   \n",
       "17900                                              Sorry  negative   \n",
       "17901   I feel like drinking wine, but I don`t have any.   neutral   \n",
       "17902                                  you are so right!  positive   \n",
       "17903                                          In denial  negative   \n",
       "17904           not yet. im still loading it.  have you?   neutral   \n",
       "17905                                             hungry  negative   \n",
       "17906              I hope for a speedy recovery for you!  positive   \n",
       "17907  is starting her pre europe diet today  goodbye...   neutral   \n",
       "17908  kid`s class is going on field trip today ~ and...   neutral   \n",
       "17909                              , is pray for you....  positive   \n",
       "17910  thankfully it`s just a really bad sinus infect...   neutral   \n",
       "17911  is eating lunch at Sushi Tei, Sency yum! met G...   neutral   \n",
       "17912                 ****. She got it on the first try.   neutral   \n",
       "17913                           Bummed I won`t be there.  negative   \n",
       "17914                                               miss  negative   \n",
       "17915                                      i`m so hungry  negative   \n",
       "\n",
       "                                             clean_tweet prediction_r  \n",
       "17886   Heh heh heh, come on! It is a THQ release! Ah...     positive  \n",
       "17887   My friend Cliff has the tix, so I have to wai...      neutral  \n",
       "17888   haha that`s because you also look amazing in ...     positive  \n",
       "17889  _Stained http://twitpic.com/4jhe5 - I LOVE it!...     positive  \n",
       "17890                              ive finished them now      neutral  \n",
       "17891                       sorry to hear about your dog     negative  \n",
       "17892  After a week staying with my Grandmother, I`m ...      neutral  \n",
       "17893   since I`m reading the Twilight series and wat...     positive  \n",
       "17894  Bummed out I am missing a rock climbing trip n...     negative  \n",
       "17895             lamentablemente paso  #jrztwitterlunch      neutral  \n",
       "17896   i`m having a hard time using hulu  have u hea...     negative  \n",
       "17897  is about to go duster shoppping in greenhills ...      neutral  \n",
       "17898  says SPACE AND TIME IS ALL WE NEED  http://plu...      neutral  \n",
       "17899  BREAKING NEWS: GM shares are currently trading...     positive  \n",
       "17900  Sorry I`m not using tweetdeck it`s not letting...     negative  \n",
       "17901   I feel like drinking wine, but I don`t have any.     negative  \n",
       "17902                                 you are so right!      positive  \n",
       "17903  In denial about the moving van parked a few bl...     negative  \n",
       "17904           not yet. im still loading it.  have you?      neutral  \n",
       "17905             im so hungry and i have nothing to eat     negative  \n",
       "17906              I hope for a speedy recovery for you!     positive  \n",
       "17907  is starting her pre europe diet today  goodbye...     positive  \n",
       "17908  kid`s class is going on field trip today ~ and...     positive  \n",
       "17909           Well, all I can do, is pray for you.....     positive  \n",
       "17910   thankfully it`s just a really bad sinus infec...     positive  \n",
       "17911  is eating lunch at Sushi Tei, Sency yum! met G...     positive  \n",
       "17912                 ****. She got it on the first try.      neutral  \n",
       "17913  Just loaned out a set of aerobars to team mate...     negative  \n",
       "17914   like cheesecake brownies!!! i miss my cheesec...     positive  \n",
       "17915                                      i`m so hungry      neutral  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_17k = df['prediction_r' != NaN]\n",
    "df_17k = df[df['prediction_r'].notna()]\n",
    "df_17k.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa accuracy: 0.7065193123465059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(df_17k['sentiment'], df_17k['prediction_r'])\n",
    "\n",
    "print(f\"RoBERTa accuracy: {accuracy}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that accuracy for RoBERTa is about 0.70 which is not much better than SVM. But I beleive with more fine tuning, RoBERTa will much better outperform SVM. \n",
    "\n",
    "Thus I believe continuing with the RoBERTa model is the right choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
